{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e382b",
   "metadata": {},
   "source": [
    "This notebook uses Roberta to generate a single dictionary which contains a mapping between a token (as a string) and a 756 dimensional averaged embedding over the provided text. The corpus to be used must be placed in the same directory as this notebook and named 'dataset.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset, sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 47820302 words.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        sentences += [line]\n",
    "        linecount += 1\n",
    "        wordcount += len(line.split())\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008bcd9",
   "metadata": {},
   "source": [
    "Quick sanity check to ensure tokenizer is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cfa809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pictures\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[1]\n",
    "tokens = tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\tpadding=\"max_length\", max_length=500, truncation=True)\n",
    "ids = tokens['input_ids'][0]\n",
    "mask = tokens['attention_mask'][0] \n",
    "print(tokenizer.decode(ids[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4d75c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46400f",
   "metadata": {},
   "source": [
    "We'll be handling tokenization in a Dataset so we can take advantage of the DataLoader for auto batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a70e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RobertaDataset(Dataset):\n",
    "\tdef __init__(self, sentances: list, tokenizer_instance: object, max_length: int):\n",
    "\t\tself.tokenizer = tokenizer_instance\n",
    "\t\tself.max_length = max_length\n",
    "\t\tself.sentences = sentences\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tsentence = self.sentences[index]\n",
    "\t\ttokens = self.tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\t\t\t padding=\"max_length\", max_length=self.max_length, truncation=True)\n",
    "\t\tids = tokens['input_ids'][0]\n",
    "\t\tmask = tokens['attention_mask'][0] \n",
    "\t\treturn (torch.LongTensor(ids), torch.LongTensor(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73684ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RobertaDataset(sentences, tokenizer, 500)\n",
    "BATCH_SIZE = 256\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5210ae",
   "metadata": {},
   "source": [
    "## Embedding Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Calculate a single embedding just to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cdf7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "ids = batch[0].to(device)\n",
    "mask = batch[1].to(device)\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(ids, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7db6",
   "metadata": {},
   "source": [
    "Calculate the embeddings of our batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005728575184752144% complete. 1894 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011457150369504288% complete. 3261 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01718572555425643% complete. 4361 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022914300739008575% complete. 5483 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028642875923760722% complete. 6369 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03437145110851286% complete. 7185 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040100026293265006% complete. 8024 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04582860147801715% complete. 8783 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0515571766627693% complete. 9449 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057285751847521445% complete. 10052 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06301432703227358% complete. 10649 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06874290221702573% complete. 11175 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07447147740177787% complete. 11748 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08020005258653001% complete. 12188 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08592862777128216% complete. 12729 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0916572029560343% complete. 13236 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09738577814078644% complete. 13765 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1031143533255386% complete. 14174 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10884292851029073% complete. 14551 tokens processed.\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence_embedding)):\n\u001b[1;32m     25\u001b[0m \ttoken \u001b[38;5;241m=\u001b[39m ids[sentence_embedding_index][token_index]\n\u001b[0;32m---> 26\u001b[0m \ttoken_str \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(token)\n\u001b[1;32m     27\u001b[0m \ttoken_embedding \u001b[38;5;241m=\u001b[39m sentence_embedding[token_index]\n\u001b[1;32m     29\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m token_str \u001b[38;5;129;01min\u001b[39;00m token_to_avg_embedding_map:\n",
      "File \u001b[0;32m~/anaconda3/envs/cs396/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4002\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3982\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m   3983\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m   4000\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4001\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m-> 4002\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m   4004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   4005\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   4006\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   4007\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   4008\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4009\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cs396/lib/python3.12/site-packages/transformers/utils/generic.py:275\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[0;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m framework_to_py_obj[framework](obj)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mnumber):\n",
      "File \u001b[0;32m~/anaconda3/envs/cs396/lib/python3.12/site-packages/transformers/utils/generic.py:260\u001b[0m, in \u001b[0;36mto_py_obj.<locals>.<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_py_obj\u001b[39m(obj):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     framework_to_py_obj \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np\u001b[38;5;241m.\u001b[39masarray(obj)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    264\u001b[0m     }\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mdict\u001b[39m, UserDict)):\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_sentances = 0\n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "model.eval()\n",
    "token_to_avg_embedding_map = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor batch in dataloader:\n",
    "\n",
    "\t\tids = batch[0].to(device)\n",
    "\t\tmask = batch[1].to(device)\n",
    "\n",
    "\t\toutput = model(ids, mask)\n",
    "\n",
    "\t\t####################################################################\n",
    "\t\n",
    "\t\t### shape, [batch, tokens in sentance, embeddings of each token]\n",
    "\t\tembeddings = output[0].detach()\n",
    "\t\t\n",
    "\t\t# Update average embeddings, \n",
    "\t\tfor sentence_embedding_index in range(len(embeddings)):\n",
    "\t\t\tsentence_embedding = embeddings[sentence_embedding_index]\n",
    "\n",
    "\t\t\tfor token_index in range(len(sentence_embedding)):\n",
    "\t\t\t\ttoken = ids[sentence_embedding_index][token_index]\n",
    "\t\t\t\ttoken_str = tokenizer.decode(token)\n",
    "\t\t\t\ttoken_embedding = sentence_embedding[token_index]\n",
    "\n",
    "\t\t\t\tif token_str in token_to_avg_embedding_map:\n",
    "\t\t\t\t\ttoken_to_avg_embedding_map[token_str] += token_embedding\n",
    "\t\t\t\t\ttoken_to_avg_embedding_map[token_str] /= 2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = token_embedding\n",
    "\t\t\n",
    "\t\tprocessed_sentances += BATCH_SIZE\n",
    "\t\tprint(f\"{processed_sentances / len(sentences) * 100.0}% complete. {len(token_to_avg_embedding_map)} tokens processed.\\t\\t\\t\", end ='\\r')\n",
    "\t\t\n",
    "\n",
    "\t\t## Cuda's leaking memeory and this seems to help?\n",
    "\t\tdel ids\n",
    "\t\tdel mask \n",
    "\t\tdel output\n",
    "\t\tdel embeddings\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\timport gc\n",
    "\t\tgc.collect()\n",
    "\t\t# print(\"gc:\\n\")\n",
    "\t\t# for obj in gc.get_objects():\n",
    "\t\t# \ttry:\n",
    "\t\t# \t\tif torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "\t\t# \t\t\tprint(type(obj), obj.size())\n",
    "\t\t# \texcept:\n",
    "\t\t# \t\tpass\n",
    "\t\t# print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : 18382\n",
      "embedding tensor([-6.7960e-02,  1.8660e-01, -1.4361e-01,  7.2157e-02, -9.8492e-02,\n",
      "         2.4160e-01,  5.6699e-02, -4.1833e-02, -7.1487e-02, -7.0940e-02,\n",
      "         5.5407e-02, -2.5917e-01, -1.7565e-01,  1.3531e-01, -2.9793e-01,\n",
      "        -6.4282e-01,  1.6737e-01, -4.7400e-02, -4.4321e-02, -1.3563e-01,\n",
      "        -2.8828e-01, -2.1811e-02, -2.6930e-01,  1.4016e-01, -6.2870e-02,\n",
      "        -1.4870e-01, -1.0622e-01, -7.9012e-02,  3.6224e-02,  8.8775e-02,\n",
      "        -8.6289e-02,  1.8704e-01,  1.7647e-02,  1.4020e-01, -1.1914e-02,\n",
      "         1.5171e-01,  1.1326e-01, -9.6438e-02, -2.4445e-01,  8.8952e-02,\n",
      "         2.4561e-02, -9.2861e-01, -1.5765e-01, -1.2588e-01, -5.9391e-03,\n",
      "         2.3183e-01, -1.8782e-01, -6.5659e-01,  9.0372e-02, -1.4454e-01,\n",
      "        -7.5906e-02,  3.7736e-01, -4.2175e-02,  3.9214e-01,  4.0338e-02,\n",
      "        -1.0264e-01,  2.0975e-02, -3.3435e-01, -1.9228e-01, -2.0601e-01,\n",
      "         2.9846e-01,  1.2260e+00, -1.3871e-01, -4.1784e-01,  2.6139e-01,\n",
      "        -4.4210e-01, -1.3612e-02, -4.2506e-01,  2.8870e-02, -1.2859e-01,\n",
      "        -1.5748e-01, -4.3087e-02,  2.2409e-01, -3.7032e-01, -1.2612e-01,\n",
      "        -1.0150e-01,  1.9111e-01, -6.3740e+00, -3.5477e-01,  1.3269e-01,\n",
      "         3.1347e-02, -1.1764e-01,  8.3803e-01,  9.3706e-02, -1.3631e-02,\n",
      "        -3.1163e-01,  1.2142e-01,  2.3402e-02,  5.7372e-02,  3.5044e-01,\n",
      "         3.1221e-01,  1.6528e-01, -3.4241e-02,  2.9379e-02,  4.2882e-01,\n",
      "         2.1061e-01,  1.5703e-02,  1.8824e+00, -1.8970e-01,  6.9884e-02,\n",
      "         8.7873e-02, -1.7675e-01, -4.5633e-01, -2.8098e-01, -1.5953e-01,\n",
      "        -2.3974e-01, -1.1176e-01, -4.0564e-01, -1.3987e-01, -2.7481e-01,\n",
      "         6.6258e-02,  2.8379e-02,  6.1077e-02,  1.3946e-01,  2.1731e-02,\n",
      "         7.1819e-01, -6.6350e-02,  1.9780e-01,  1.5199e-01,  2.4097e-01,\n",
      "        -1.4797e-01,  1.2739e-01,  1.6399e-01, -1.4564e-01, -1.8111e-01,\n",
      "        -1.6489e-01, -1.6894e-01, -1.1832e-01,  1.3741e-01, -4.1460e-03,\n",
      "         3.7370e-01,  4.1493e-02, -8.6453e-02,  3.8803e-01,  2.4104e-01,\n",
      "         2.3962e-01, -7.5133e-02,  1.7560e-02,  1.0942e-01,  2.2047e-02,\n",
      "         7.4174e-03,  2.7972e-01,  1.7858e-02, -2.7038e-01, -4.0760e-02,\n",
      "         9.5874e-02,  2.5303e-01, -1.3876e-01, -9.2458e-02, -7.9350e-02,\n",
      "        -9.0625e-02,  5.6728e-02,  4.2275e-02, -8.3955e-02,  2.0468e-01,\n",
      "        -6.7349e-03,  2.6702e-01,  6.7198e-03,  2.6876e-01,  4.2457e-01,\n",
      "         2.2934e-01,  3.4987e-02,  1.0788e-01, -1.5239e-01,  4.3590e-02,\n",
      "        -1.4468e-01,  1.9122e-01,  8.0018e-02,  1.0948e-01,  3.5096e-02,\n",
      "         8.6203e-02,  2.6029e-01, -7.4038e-02,  5.1422e-02,  4.0663e-02,\n",
      "        -1.5288e-01, -2.0506e-02,  4.0194e-02,  1.6352e-02,  1.2000e-01,\n",
      "         4.1362e-01,  2.6725e-01,  1.3416e-01, -2.5208e-01,  4.0873e-01,\n",
      "         1.8342e-01,  1.8713e-01,  2.0494e-02,  1.9274e-01,  1.1522e-01,\n",
      "        -3.5944e-01,  3.1033e-01, -9.7433e-02,  7.3697e-01,  1.1849e-01,\n",
      "         2.3412e-01,  1.1310e-01,  1.0922e-01, -7.0937e-02, -4.0791e-01,\n",
      "        -6.4579e-02,  2.2010e-01,  2.6362e-01, -2.2545e-01,  1.2311e-01,\n",
      "         1.8521e-01,  6.8380e-02,  3.0954e-02,  1.2915e-02,  3.5325e-01,\n",
      "         4.8593e-02,  8.7353e-02,  1.0954e-01,  1.0802e-01, -5.4087e-01,\n",
      "         8.4746e-02, -1.2623e-01, -1.9376e+00,  5.5090e-03, -3.1135e-01,\n",
      "         3.2668e-02, -1.6243e-01,  3.9823e-02, -4.3491e-02,  1.5609e-01,\n",
      "        -1.0750e-01, -1.1023e-01,  2.3415e-01,  2.5932e-01,  6.3797e-02,\n",
      "        -2.9658e-01, -9.9272e-03,  2.0126e-02, -2.9061e-01, -6.4334e-03,\n",
      "        -4.7589e-01,  7.6989e-02, -7.0940e-02,  2.0821e-01, -8.7323e-02,\n",
      "         9.5639e-01, -9.5179e-02, -4.1126e-02,  2.8433e-01, -4.7476e-02,\n",
      "        -1.6219e-01,  8.1025e-02,  7.8689e-01,  2.3951e-02,  1.1582e-01,\n",
      "         2.3710e-01,  2.2387e-01,  2.1613e-02, -1.2224e-01, -1.5862e-01,\n",
      "         1.2062e-01,  1.6772e-01, -1.3627e-01, -3.0544e-01, -1.1951e-01,\n",
      "        -1.4957e-02,  1.5725e-01, -1.2627e-01, -3.2663e-01, -9.9403e-02,\n",
      "        -1.9741e-01, -5.9025e-02, -1.4899e-01,  5.7367e-02,  3.3700e-02,\n",
      "         3.1438e-01,  2.7161e-01,  5.0419e-02, -5.4087e-02,  9.5672e-02,\n",
      "         1.9773e-01,  1.4277e-01, -1.7054e-01,  4.2362e-04, -7.4119e-02,\n",
      "        -1.7086e-02,  1.7691e-01, -2.7847e-02,  3.1826e-02,  4.2129e-03,\n",
      "         2.8670e-01,  9.1498e-01, -4.5569e-01, -3.2696e-01, -3.2988e-01,\n",
      "         2.7598e-01,  9.6874e-02, -2.1671e-01,  1.5448e-02, -1.3947e-01,\n",
      "        -1.1818e-02,  2.1826e-01, -2.1288e-01,  5.6344e-02,  3.4310e-01,\n",
      "         1.2206e-01, -1.6247e-01, -2.2062e-02,  3.1575e-03, -2.0282e-01,\n",
      "        -2.3002e-01, -1.3331e-01,  3.0051e-01, -2.0658e-01,  2.1089e-02,\n",
      "        -8.4949e-02, -5.8834e-02, -9.8270e-02,  1.1090e-01,  1.4751e-01,\n",
      "         2.2153e-01,  7.5319e-02,  2.8255e-01,  1.7369e-01, -1.1319e-01,\n",
      "        -7.9030e-02,  3.0397e-01, -4.1922e-01,  5.1071e-02, -3.5887e-01,\n",
      "         2.8308e-01, -4.0067e-01, -3.4582e-02, -2.4430e-01,  1.8872e-01,\n",
      "         2.8971e-01,  1.3112e+00, -3.7823e-01,  7.4668e-01,  7.2290e-02,\n",
      "         3.5368e-02,  3.5061e-01, -5.1434e-02, -4.9914e-02,  8.6575e-02,\n",
      "        -5.4405e-02, -1.3861e-01, -6.6413e-03,  5.6103e-02, -1.2813e-01,\n",
      "         1.3098e-01, -7.0700e-02,  1.5091e-01, -3.1920e-02,  2.1000e-01,\n",
      "         1.6660e-01, -1.2610e-01, -1.4664e-01,  6.4883e-02, -3.1030e-01,\n",
      "         3.6479e-01, -6.2412e-02, -4.7102e-02,  3.2316e-02,  5.6496e-01,\n",
      "         7.4168e-02,  2.0004e-01,  4.1182e-02,  2.9385e-01, -6.3440e-02,\n",
      "        -2.3737e-01, -5.5049e-01, -1.4401e-01,  1.0717e-01,  1.5588e-01,\n",
      "        -1.5752e-01, -1.2878e-01, -8.7243e-02,  1.6554e-01,  1.0956e-01,\n",
      "         2.0553e-01, -1.7610e-01, -1.6339e-01, -2.0866e-02, -1.3947e-01,\n",
      "         2.0903e-01,  4.0318e-01,  1.5632e-01,  6.8635e-02,  2.3644e-01,\n",
      "        -1.8249e-01,  3.0165e-01, -3.1927e-01, -1.0751e-01,  1.6318e-01,\n",
      "         8.9765e-02,  3.1043e-02,  1.0050e-01, -1.9224e-01, -6.5719e-02,\n",
      "        -4.5750e-02,  8.0184e-02, -5.8606e-01,  5.3674e-02,  6.4475e-02,\n",
      "         1.9992e-01,  1.7225e-01,  2.1078e-01,  4.0118e-01,  1.0526e-01,\n",
      "        -2.8608e-01, -3.3394e-01, -1.2646e-01,  2.9053e-02, -5.7339e-02,\n",
      "        -4.4497e-02,  3.5774e-02,  4.8614e-02,  5.2846e-02,  1.1171e-01,\n",
      "        -1.3219e-01,  3.4596e-01,  3.3717e-01,  3.6006e-01, -6.8775e-02,\n",
      "         1.1183e-01,  2.1328e-01,  4.0175e-01,  1.5401e-01,  5.1731e-01,\n",
      "         2.3588e-01,  2.0257e-01,  8.3527e-02, -4.6581e-02,  1.8915e-01,\n",
      "         2.4127e-01,  1.4729e-01, -1.1806e-01,  1.1972e-01, -3.5089e-01,\n",
      "        -1.0839e-01, -5.8647e-02, -1.5022e-01, -3.8223e-01, -1.1408e-01,\n",
      "        -5.5066e-02, -3.7903e-02,  3.1345e-01,  4.3993e-01, -2.6486e-02,\n",
      "         1.3883e-01, -1.6053e-01,  2.5461e-01,  3.6959e-01,  2.8099e-01,\n",
      "        -3.1618e-02, -2.9726e-02,  7.7346e-03, -1.2508e+00, -5.5484e-02,\n",
      "         3.6825e-02,  1.4855e-01,  1.3406e-01, -9.8146e-02, -1.3787e-01,\n",
      "        -4.9019e-02, -1.0029e-01,  1.2379e-01, -8.5398e-02,  2.9857e-01,\n",
      "        -2.0302e-01, -1.0471e-01,  3.2542e-02,  7.6960e-02,  2.6634e-01,\n",
      "        -3.3991e-01, -9.7161e-02,  1.3680e-01, -3.1021e-01, -2.3131e-02,\n",
      "         6.8469e-02, -1.6847e-01, -8.9357e-02,  6.0185e-02, -9.0394e-02,\n",
      "         3.4345e-01,  2.0050e-01, -1.7190e-02,  9.8344e-02,  1.2184e-01,\n",
      "        -1.2607e-01,  1.4085e-01,  3.3255e-03,  1.5266e-01, -4.6252e-02,\n",
      "         9.1293e-02, -1.2653e-01, -1.1253e-01,  1.1244e-01,  1.6507e-01,\n",
      "        -1.3388e-02, -4.9033e-01, -2.6029e-01, -7.5836e-01,  2.2358e-01,\n",
      "         2.5711e-01,  3.2923e-01,  6.5561e-03,  3.1067e-01, -7.2608e-02,\n",
      "        -1.5816e-01, -3.3749e-01, -2.0900e-01, -8.5784e-02, -2.3405e-01,\n",
      "         1.6344e-01,  2.2010e-01, -1.0969e-01, -3.4293e-01, -1.7549e-01,\n",
      "        -7.2241e-02,  6.1676e-02, -3.2561e-01, -5.0740e-02, -7.8024e-01,\n",
      "        -1.6699e-01, -6.9299e-02,  2.0002e-02,  1.5339e-01,  8.1141e-02,\n",
      "         1.5999e-01,  7.5678e-02, -6.5131e-02, -1.5158e-01,  1.7685e-01,\n",
      "        -1.8804e-01,  1.4884e-01, -2.2138e-01, -3.8038e-02, -4.7320e-01,\n",
      "         4.1172e-02,  1.0544e-01,  3.4076e-02, -1.5246e-01,  1.3933e-01,\n",
      "         2.8044e-01,  6.0663e-01,  3.0265e-01, -3.9910e-01,  1.0575e-01,\n",
      "        -1.4264e-01, -1.7577e-01, -1.1265e-01, -4.9522e-03,  1.7764e-01,\n",
      "         2.5757e-02, -5.3832e-01,  2.1574e-02, -1.1348e-02,  1.9602e-01,\n",
      "         6.4567e-02, -2.0551e-01,  2.4855e-02, -7.9829e-03,  2.4574e-01,\n",
      "         1.1742e-01, -4.6019e-01, -2.1766e-01, -7.1702e-02, -2.6656e-01,\n",
      "        -2.4796e-01,  2.3994e-02, -1.2562e-01, -5.9311e-02,  1.7980e-01,\n",
      "        -1.0543e+00,  1.3395e-01,  8.2541e-02, -1.2617e-01,  8.6285e-02,\n",
      "         9.8848e-02,  1.2915e-01,  1.0894e-01, -2.1840e-01,  4.2174e-02,\n",
      "         1.6057e-01,  2.2102e-01, -3.2841e-01, -2.2825e-01, -2.0158e-02,\n",
      "         7.1708e-02, -1.2570e-01, -1.5190e-01,  8.3564e+00, -2.3433e-01,\n",
      "         2.5227e-01,  1.9738e-01, -5.0856e-02,  2.0378e-01, -2.1321e-01,\n",
      "         9.5624e-02, -1.6306e-01,  1.1288e-01, -5.3876e-02,  4.3996e-01,\n",
      "        -1.1829e-01,  1.4878e-01,  2.5560e-01, -1.4808e-02,  1.5658e-01,\n",
      "         3.6291e-01, -8.7974e-02,  9.6667e-02,  4.2547e-02,  7.1726e-01,\n",
      "         6.0230e-03,  1.4652e-01, -1.2652e-01, -2.3466e-01,  3.5669e-01,\n",
      "         1.2215e-01, -2.8003e-01,  5.4310e-02, -1.1533e-01,  2.3431e-01,\n",
      "         9.4463e-02, -1.0396e-01,  1.7959e-01, -2.9284e-01,  7.3108e-01,\n",
      "        -4.9151e-02,  7.3687e-02, -4.5642e-03, -2.3085e-02, -9.1348e-02,\n",
      "         1.1952e-01, -3.4335e-02, -1.9989e-01, -5.2142e-03, -1.3553e-01,\n",
      "         6.2244e-02,  2.0850e-01, -1.0942e-02,  2.8073e-01,  5.1964e-02,\n",
      "         9.8719e-03, -1.4313e-01,  8.0853e-02,  3.0437e-01, -1.2036e-01,\n",
      "        -1.0473e-01,  2.8628e-01, -8.8916e-03,  1.9746e-04,  3.3111e-01,\n",
      "         5.9204e-02,  1.0721e-01,  2.1955e-01,  2.9540e-01, -2.4195e-01,\n",
      "         2.9348e-01,  4.2094e-01,  4.3417e-02, -6.2682e-03, -2.0887e-01,\n",
      "         2.0650e-01, -9.2119e-02,  8.7365e-02,  6.4348e-02, -1.0670e+00,\n",
      "        -3.8446e-02, -2.0130e-01, -4.2089e-02, -1.9699e-02,  1.9323e-01,\n",
      "        -1.7669e-01, -5.0782e-03,  4.9018e-01,  2.1904e-01,  8.4420e-02,\n",
      "        -1.7611e-01, -3.6601e-01,  3.1026e-01,  1.1344e-02, -5.5804e-02,\n",
      "         1.1271e-01, -1.3746e-01,  2.6595e-02, -3.0495e-01, -3.0766e-02,\n",
      "        -2.4409e-01,  1.6478e-02, -1.7113e-01,  2.4724e-01,  6.2128e-02,\n",
      "        -3.8503e-02,  1.5398e-01,  1.0184e-01,  5.7750e-02,  3.0500e-01,\n",
      "        -9.6698e-02,  3.5724e-01, -5.4792e-02, -1.0166e-01,  3.3088e-01,\n",
      "        -1.3737e-01, -1.0131e-01,  2.4397e-02, -3.2883e-02,  2.8838e-01,\n",
      "         3.1476e-01,  1.9699e-01, -2.5686e-02,  2.2640e-01, -2.7358e-01,\n",
      "        -1.9961e-01,  1.7149e-01,  2.5270e-01,  1.6110e-02, -1.0626e-01,\n",
      "         1.9807e-01,  1.3341e-02, -3.0454e-02,  1.7330e-02,  1.0347e-01,\n",
      "         2.7416e-01,  1.1434e-01, -2.7360e-01, -7.9949e-02,  3.0045e-02,\n",
      "        -1.4290e-01,  9.3726e-02,  1.7210e-01,  9.6522e-02,  2.0918e-01,\n",
      "        -9.9402e-02, -1.4817e-01,  3.7383e-02, -2.1766e-01, -4.7892e-01,\n",
      "         1.2511e-01, -2.0641e-01, -3.6465e-01,  3.0606e-02,  4.8113e-01,\n",
      "        -2.3076e-01, -1.0163e-01,  7.0632e-02, -1.7654e-01, -1.2310e-01,\n",
      "         4.1397e-02,  1.3078e-01,  2.2691e-01,  5.9116e-01, -8.5220e-01,\n",
      "         3.1039e-01, -1.5862e-01, -6.8228e-01,  3.2162e-02,  1.6099e-01,\n",
      "         3.0720e-01,  1.2829e-01, -5.8537e-02, -2.0771e-01,  2.1776e-01,\n",
      "        -9.6896e-03,  1.6066e-01,  1.5403e-01,  8.4657e-02, -2.8619e-01,\n",
      "         6.0836e-01, -8.4621e-02,  1.2645e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens : \" + str(len(token_to_avg_embedding_map)))\n",
    "print(\"embedding \" + str(token_to_avg_embedding_map[\"red\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
