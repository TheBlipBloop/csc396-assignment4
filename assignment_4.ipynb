{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "random seed: 1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bloop/anaconda3/envs/cs396/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647329220/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 300935040 words.\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        dataset += line.split()\n",
    "        linecount += 1\n",
    "        wordcount += len(line)\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")\n",
    "\n",
    "\n",
    "# Minimize dataset for testing,\n",
    "dataset = dataset[:128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer and then prepare the dataset by A) Generating a tokenization and B) Generating a vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cb104-5f27-41a4-a61e-f11ec2e8c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'White', 'Monkey', 'is', 'a', '1925', 'American', 'silent', 'drama', 'film,']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2bbba3a5ae4972be6fb3d833321727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(  0%|          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...64\n",
      "64...128\n",
      "processed tokens : 170\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:10])\n",
    "chunk_size = 64\n",
    "tokens = []\n",
    "\n",
    "for i in tqdm(range(len(dataset) // chunk_size)):\n",
    "    sequence_start = i * chunk_size\n",
    "    sequence_end = (i + 1) * chunk_size\n",
    "    tokens += tokenizer(dataset[sequence_start:sequence_end], is_split_into_words=True)[\"input_ids\"]\n",
    "    print(str(sequence_start) + \"...\" + str(sequence_end))\n",
    "\n",
    "print(\"processed tokens : \" + str(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Take tokens and shtick em in the model to get the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e876f1-9cfb-42c8-aa9b-430029cc8adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1162,  0.1060, -0.0101,  ..., -0.0897, -0.0220, -0.0182],\n",
       "         [ 0.0594, -0.0531, -0.0033,  ..., -0.2571,  0.0529,  0.4554],\n",
       "         [ 0.0060,  0.2149,  0.0754,  ...,  0.0744,  0.1305,  0.0811],\n",
       "         ...,\n",
       "         [-0.1735,  0.0330,  0.0861,  ...,  0.2404,  0.2087, -0.0735],\n",
       "         [-0.1080,  0.1013, -0.0331,  ..., -0.1196, -0.0253, -0.0431],\n",
       "         [ 0.0159,  0.0289,  0.0789,  ...,  0.1065,  0.0235,  0.0473]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.0287,  0.1055,  0.0847, -0.0798, -0.2726,  0.0077, -0.0934, -0.1399,\n",
       "          0.0172, -0.0689,  0.1391,  0.1346, -0.1971, -0.1770, -0.2654,  0.0451,\n",
       "         -0.0320, -0.0280,  0.2640, -0.0239, -0.0305,  0.0018,  0.3647,  0.0869,\n",
       "          0.1374, -0.0802, -0.0496, -0.3941, -0.0076,  0.1177,  0.1581,  0.0949,\n",
       "         -0.0519, -0.0592,  0.1684,  0.1593,  0.3215, -0.7168,  0.1521,  0.2916,\n",
       "         -0.2329,  0.0027, -0.3044,  0.1198, -0.2539, -0.2008, -0.1040, -0.2427,\n",
       "         -0.0604,  0.3060,  0.2578,  0.2411, -0.1485, -0.1330,  0.3047,  0.3191,\n",
       "         -0.0470,  0.1767,  0.0231,  0.0385, -0.6244,  0.0793, -0.1426,  0.0273,\n",
       "         -0.1563,  0.3387, -0.0945,  0.2001, -0.1942, -0.5323, -0.0944, -0.1410,\n",
       "         -0.0995, -0.1937,  0.1150, -0.0254, -0.0705, -0.1981, -0.1535,  0.4201,\n",
       "          0.1178,  0.1857,  0.0571,  0.1788,  0.3297,  0.1335, -0.0433, -0.0164,\n",
       "         -0.0040, -0.1670, -0.0407, -0.2121, -0.0849, -0.0512, -0.0745, -0.1798,\n",
       "          0.1467,  0.0447,  0.1938,  0.3928, -0.3688,  0.0534,  0.1736,  0.0168,\n",
       "          0.1446,  0.2796,  0.0344,  0.1444, -0.4668, -0.0437,  0.1305, -0.4337,\n",
       "          0.1114,  0.0904,  0.2784, -0.1934, -0.1416,  0.1476,  0.3821, -0.0483,\n",
       "          0.2091, -0.0271,  0.1569,  0.0222,  0.3279, -0.2772, -0.4115,  0.2072,\n",
       "         -0.1671,  0.0348,  0.1248,  0.2790, -0.0313, -0.0069,  0.2266, -0.1839,\n",
       "          0.1765,  0.0960,  0.0043,  0.0958, -0.4764,  0.1170, -0.3291, -0.0939,\n",
       "          0.0311,  0.4197,  0.2219, -0.1502, -0.1906,  0.0876,  0.0520,  0.1843,\n",
       "          0.0269,  0.0465, -0.2885, -0.0541, -0.1744,  0.1930, -0.4440,  0.1239,\n",
       "          0.2867,  0.0782,  0.0242,  0.1483, -0.0634, -0.3197,  0.3281, -0.4324,\n",
       "         -0.0910,  0.1440, -0.2467, -0.2601, -0.3161, -0.1529,  0.0777, -0.0385,\n",
       "         -0.2364,  0.2073,  0.1433,  0.1671, -0.0062,  0.2369,  0.2261,  0.2636,\n",
       "          0.0108, -0.3003,  0.0473,  0.3207, -0.3601, -0.3576,  0.0641,  0.2124,\n",
       "          0.1659,  0.0329, -0.4000,  0.1591,  0.0503, -0.0327,  0.0346, -0.1806,\n",
       "         -0.0109,  0.1081,  0.1120, -0.0207, -0.1702, -0.1183, -0.3313,  0.5944,\n",
       "         -0.0466, -0.2692, -0.0491,  0.0800, -0.2261, -0.5011,  0.3625,  0.3431,\n",
       "         -0.4073,  0.1410, -0.0152,  0.3207,  0.3196,  0.2028, -0.1256, -0.2990,\n",
       "         -0.0268,  0.2999,  0.0009, -0.0314,  0.1037,  0.1059, -0.1894,  0.3992,\n",
       "          0.0034,  0.2374,  0.1538,  0.2457, -0.1056, -0.3853,  0.0588,  0.0224,\n",
       "         -0.0816, -0.0847, -0.3485, -0.3452,  0.2577,  0.1173, -0.1181,  0.1442,\n",
       "          0.2191, -0.1407, -0.0535,  0.0187,  0.4718, -0.1851,  0.0317,  0.2622,\n",
       "          0.1795,  0.4608, -0.0080, -0.2022, -0.5678, -0.0117, -0.4198, -0.0742,\n",
       "          0.2984,  0.0071, -0.3721,  0.2535, -0.3676,  0.1358,  0.0831,  0.0474,\n",
       "          0.3675,  0.2703, -0.0779, -0.3477,  0.1058, -0.2428, -0.1121,  0.0920,\n",
       "         -0.1559, -0.1785, -0.1805,  0.1014, -0.1006, -0.1806,  0.0938, -0.2397,\n",
       "         -0.3653, -0.0094,  0.3310,  0.0905, -0.2783, -0.0311,  0.3415, -0.0389,\n",
       "          0.3081,  0.0440, -0.0218, -0.3207,  0.4200,  0.1050,  0.2868,  0.3191,\n",
       "         -0.0092,  0.0556, -0.1923,  0.0698,  0.1287,  0.0612, -0.1881,  0.2068,\n",
       "         -0.0945,  0.5440, -0.2229,  0.1293,  0.0940, -0.0251,  0.0698,  0.1801,\n",
       "          0.3894, -0.1428, -0.3071,  0.0877, -0.0804, -0.0253,  0.3194, -0.1686,\n",
       "          0.0569,  0.2303, -0.0707,  0.0252, -0.0857,  0.1149, -0.4904, -0.3244,\n",
       "         -0.2700, -0.2031,  0.3410, -0.2223,  0.2535,  0.2648, -0.0075,  0.1108,\n",
       "         -0.1370, -0.2735,  0.3128, -0.1267, -0.2626, -0.0859,  0.0999, -0.0877,\n",
       "          0.0252,  0.0693, -0.3651,  0.3280,  0.0534, -0.2889, -0.3417, -0.1715,\n",
       "         -0.0674, -0.0653, -0.1600,  0.0292, -0.1106, -0.1005, -0.1440, -0.3424,\n",
       "         -0.0562,  0.1264, -0.2126,  0.2569,  0.1266,  0.1642,  0.1085,  0.0804,\n",
       "          0.1224,  0.1836, -0.1128,  0.0287,  0.2335, -0.0932, -0.0219, -0.2962,\n",
       "         -0.1519, -0.0609, -0.0758,  0.1691,  0.0636, -0.2437,  0.1340, -0.0438,\n",
       "         -0.0026,  0.1968,  0.3131,  0.1114, -0.0233,  0.0966,  0.4063, -0.1460,\n",
       "         -0.2577,  0.1776,  0.1113, -0.1746,  0.0849,  0.0883, -0.4689, -0.4832,\n",
       "         -0.1121, -0.1010,  0.0951,  0.2618,  0.0537,  0.5281,  0.4153, -0.5810,\n",
       "          0.2004, -0.0845, -0.1458, -0.1378,  0.0411,  0.2129, -0.1560,  0.0480,\n",
       "          0.1174, -0.0174,  0.1457, -0.2375, -0.2541, -0.4609, -0.1697, -0.1845,\n",
       "          0.1361, -0.1553, -0.2032, -0.0286, -0.1443,  0.0381,  0.2145,  0.3446,\n",
       "          0.0548, -0.1506,  0.2248, -0.1934,  0.0673,  0.1694,  0.0114,  0.2322,\n",
       "          0.5963, -0.3487,  0.2564, -0.2131,  0.1251,  0.0617,  0.3602, -0.5622,\n",
       "          0.4187, -0.0367, -0.4649,  0.2486,  0.0537, -0.1494, -0.2669,  0.2186,\n",
       "         -0.1995,  0.4731, -0.0208,  0.0106, -0.2287,  0.4580,  0.3176, -0.2552,\n",
       "          0.0452,  0.2858,  0.1707,  0.0474,  0.5154, -0.1739,  0.1292, -0.3297,\n",
       "         -0.4606,  0.1583,  0.0841,  0.1006,  0.4772,  0.3514, -0.2052,  0.1382,\n",
       "          0.0407,  0.0761,  0.0378,  0.0645, -0.0055, -0.0417,  0.4982, -0.1126,\n",
       "         -0.2454,  0.3277,  0.2674,  0.3792, -0.1768, -0.0284, -0.6062,  0.4521,\n",
       "         -0.3732, -0.1741, -0.0436,  0.3192, -0.1314,  0.0032, -0.2016,  0.0527,\n",
       "          0.1368, -0.1764,  0.1566, -0.0379,  0.3355,  0.2658,  0.4292, -0.1027,\n",
       "          0.0936,  0.2811,  0.5106, -0.2941, -0.2543,  0.1525, -0.4146,  0.2363,\n",
       "         -0.2195, -0.2405,  0.2226, -0.1824, -0.0318,  0.0335, -0.1411, -0.2075,\n",
       "         -0.2587, -0.2017, -0.3012, -0.2828, -0.0610, -0.2268, -0.1567, -0.5687,\n",
       "         -0.1178, -0.0089,  0.0712,  0.0877, -0.4000,  0.5018,  0.2478, -0.1873,\n",
       "          0.1896,  0.2528,  0.2934,  0.2734,  0.0465, -0.4315,  0.2954,  0.1663,\n",
       "          0.2128,  0.2352,  0.0316,  0.2518,  0.4569,  0.0651,  0.5043, -0.1585,\n",
       "          0.0770,  0.0691,  0.1333,  0.1477,  0.1320, -0.0897,  0.1635, -0.2520,\n",
       "          0.1119, -0.2950, -0.1270, -0.1846, -0.3643, -0.2133,  0.1462, -0.1071,\n",
       "          0.3104,  0.0009, -0.1222,  0.0384,  0.0389,  0.1891,  0.3240, -0.0463,\n",
       "         -0.6165, -0.2704,  0.2738, -0.0696, -0.1113, -0.0397, -0.1397,  0.4050,\n",
       "          0.0104,  0.2785,  0.0184, -0.3467, -0.0502, -0.3688,  0.4153,  0.3294,\n",
       "         -0.0224, -0.0025,  0.1602, -0.3242, -0.1505,  0.4884, -0.0090, -0.3017,\n",
       "          0.2934, -0.5662, -0.2951, -0.0743, -0.4716, -0.4128,  0.0271, -0.1838,\n",
       "         -0.1032,  0.0663, -0.1975, -0.1516, -0.0409, -0.1207,  0.0665,  0.2320,\n",
       "          0.2694, -0.2406,  0.0545,  0.1053,  0.1905,  0.0463, -0.1024, -0.0540,\n",
       "          0.0324, -0.0544, -0.0685, -0.1480,  0.0438, -0.1702, -0.2106, -0.0915,\n",
       "          0.1691,  0.2666, -0.3817, -0.2553,  0.0623, -0.0911,  0.0079,  0.3531,\n",
       "          0.0823, -0.2437, -0.1171, -0.1864, -0.0840, -0.2547, -0.1391,  0.1519,\n",
       "          0.0794, -0.0353, -0.0264,  0.0534, -0.1505, -0.4481, -0.1757,  0.2817,\n",
       "         -0.1745, -0.5285, -0.2816, -0.6318,  0.3592,  0.1950,  0.0362, -0.0724,\n",
       "         -0.1769, -0.1057, -0.2522, -0.1588,  0.2454, -0.1725,  0.2043,  0.1881,\n",
       "          0.3158, -0.1366, -0.2295, -0.2098,  0.0573, -0.2333, -0.0846,  0.2270,\n",
       "         -0.2118, -0.3223, -0.0694,  0.3180, -0.1828,  0.3237, -0.3736,  0.0171,\n",
       "         -0.0276, -0.1737, -0.2428, -0.1044,  0.2723, -0.1995, -0.0161, -0.1704,\n",
       "         -0.3497, -0.4127,  0.1635, -0.2351,  0.0521,  0.3218,  0.0724, -0.2288,\n",
       "         -0.3225,  0.2969,  0.1486, -0.2092, -0.0355, -0.4794, -0.1394,  0.2150,\n",
       "          0.3901, -0.2454, -0.0294,  0.4020,  0.2172,  0.3881,  0.2019,  0.1225,\n",
       "         -0.3539, -0.2082,  0.0467,  0.1933,  0.3284,  0.1190, -0.1863,  0.0642,\n",
       "         -0.0743,  0.0691, -0.3707,  0.2041,  0.3349,  0.0521, -0.1258, -0.0599,\n",
       "          0.0734,  0.1759,  0.5082,  0.1222, -0.0507,  0.4342,  0.0850,  0.3740,\n",
       "         -0.2452, -0.2365, -0.2877,  0.1609, -0.1452, -0.0986,  0.0227,  0.2974]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
