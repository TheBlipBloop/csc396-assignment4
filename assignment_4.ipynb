{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e382b",
   "metadata": {},
   "source": [
    "This notebook uses Roberta to generate a single dictionary which contains a mapping between a token (as a string) and a 756 dimensional averaged embedding over the provided text. The corpus to be used must be placed in the same directory as this notebook and named 'dataset.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report\n",
    "from operator import itemgetter\n",
    "import psutil\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset, sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 47820302 words.\n",
      "Average length: 67.34097665493726\n",
      "Max length: 3263\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0 \n",
    "\n",
    "lengths = []\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        sentences += [line]\n",
    "        linecount += 1\n",
    "        wordcount += len(line.split())\n",
    "        lengths += [len(line)]\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")\n",
    "print(\"Average length: \" + str(np.average(lengths)))\n",
    "print(\"Max length: \" + str(np.max(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a fast tokenizer,\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008bcd9",
   "metadata": {},
   "source": [
    "Quick sanity check to ensure tokenizer is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cfa809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pictures\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[1]\n",
    "tokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = 128, truncation = True, return_tensors='pt')\n",
    "\n",
    "# tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\t# padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "## Maybe reduce max length to 256 \n",
    "ids = tokens['input_ids'][0]\n",
    "mask = tokens['attention_mask'][0]\n",
    "print(tokenizer.decode(ids[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4d75c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46400f",
   "metadata": {},
   "source": [
    "We'll be handling tokenization in a Dataset so we can take advantage of the DataLoader for auto batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a70e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RobertaDataset(Dataset):\n",
    "\tdef __init__(self, sentences: list, max_length: int):\n",
    "\t\tsentences_tokenized = []\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\ttokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = max_length, truncation = True, return_tensors='pt')\n",
    "\t\t\t\n",
    "\t\t\tids = torch.LongTensor(tokens['input_ids'][0])\n",
    "\t\t\tmask = torch.LongTensor(tokens['attention_mask'][0]) \n",
    "\n",
    "\t\t\tsentences_tokenized += [np.array([ids, mask])]\n",
    "\n",
    "\t\t\tprint(f\"{len(sentences_tokenized) / len(sentences) * 100.0}% complete.\\t\\t\\t\", end ='\\r')\n",
    "\n",
    "\t\tself.sentences_tokenized = np.array(sentences_tokenized)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences_tokenized)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn (self.sentences_tokenized[index][0], self.sentences_tokenized[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73684ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete.\t\t\tcomplete.\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "dataset = RobertaDataset(sentences, 128)\n",
    "BATCH_SIZE = 256\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5210ae",
   "metadata": {},
   "source": [
    "## Embedding Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Calculate a single embedding just to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdf7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "ids = batch[0].to(device)\n",
    "mask = batch[1].to(device)\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(ids, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7db6",
   "metadata": {},
   "source": [
    "Calculate the embeddings of our batches. I abort once 12_500 tokens have been collected as I start to run out of RAM after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aborting embedding generation early to avoid running out of RAM!AM utilization. \t\t\t\t\t\t\n",
      "83.236130816 GB used.\n",
      "5.442146425514537% complete. 45429 embeddings generated. 99.6% RAM utilization.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "import sys, traceback, gc\n",
    "type, val, tb = None, None, None\n",
    "type, val, tb = sys.exc_info()\n",
    "traceback.clear_frames(tb)\n",
    "\n",
    "token_to_avg_embedding_map = dict()\n",
    "avg_token_embedding = None\n",
    "\n",
    "def calculate_embeddings() -> dict:\n",
    "\tprocessed_sentances = 0\n",
    "\t\n",
    "\tmodel = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\ttoken_to_avg_embedding_map = {}\n",
    "\tavg_token_embedding = None\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\t\n",
    "\t\t\tids = batch[0].to(device)\n",
    "\t\t\tmask = batch[1].to(device)\n",
    "\t\t\t\n",
    "\t\t\toutput = model(ids, mask)\n",
    "\t\n",
    "\t\t\t####################################################################\n",
    "\t\t\n",
    "\t\t\t### shape, [batch, tokens in sentance, embeddings of each token]\n",
    "\t\t\tembeddings = output[0].detach().cpu().numpy()\n",
    "\t\n",
    "\t\t\t\n",
    "\t\t\t# Update average embeddings, \n",
    "\t\t\tfor sentence_embedding_index in range(len(embeddings)):\n",
    "\t\t\t\tsentence_embedding = embeddings[sentence_embedding_index]\n",
    "\t\n",
    "\t\t\t\tfor token_index in range(len(sentence_embedding)):\n",
    "\t\t\t\t\ttoken = ids[sentence_embedding_index][token_index]\n",
    "\t\t\t\t\ttoken_str = tokenizer.decode(token)\n",
    "\t\t\t\t\ttoken_embedding = sentence_embedding[token_index]\n",
    "\t\n",
    "\t\t\t\t\tif avg_token_embedding is None:\n",
    "\t\t\t\t\t\tavg_token_embedding = token_embedding\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tavg_token_embedding = np.mean(np.array([avg_token_embedding, token_embedding]), axis=0)\n",
    "\t\n",
    "\t\t\t\t\tif token_str in token_to_avg_embedding_map:\n",
    "\t\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = np.mean(np.array([token_to_avg_embedding_map[token_str], token_embedding]), axis=0)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = token_embedding\n",
    "\t\t\t\n",
    "\t\t\tdel embeddings\n",
    "\t\t\tdel ids\n",
    "\t\t\tdel mask\n",
    "\t\t\n",
    "\t\t\tif (psutil.virtual_memory().percent) > 99.5:\n",
    "\t\t\t\tprint(\"Aborting embedding generation early to avoid running out of RAM!\")\n",
    "\t\t\t\tprint(f\"{psutil.virtual_memory().used / 1e9} GB used.\")\n",
    "\t\t\t\tprint(f\"{processed_sentances / len(sentences) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization.\")\n",
    "\t\t\t\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\t\t\t\n",
    "\t\t\tprocessed_sentances += BATCH_SIZE\n",
    "\t\t\tprint(f\"{processed_sentances / len(sentences) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization. \\t\\t\\t\", end ='\\r')\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\n",
    "token_to_avg_embedding_map, avg_token_embedding = calculate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cf766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4684",
   "metadata": {},
   "source": [
    "# Problem one complete!\n",
    "The token_to_avg_embedding_map is a dictonary mapping between sub-word tokens and their average embedding in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fb3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : 45429\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens : \" + str(len(token_to_avg_embedding_map)))\n",
    "avg_token_embedding = avg_token_embedding.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f5629",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "In this section we are going to implement the most_similar() functions from chp 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72312c",
   "metadata": {},
   "source": [
    "First, generate a word to embedding mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6827696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(word):\n",
    "\ttokens = tokenizer(word)['input_ids']\n",
    "\tembedding = np.array(avg_token_embedding)\n",
    "\tfor token in tokens:\n",
    "\t\ttoken = tokenizer.decode(token) # get the string this token id represents\n",
    "\t\tif token in token_to_avg_embedding_map:\n",
    "\t\t\tembedding += np.array(token_to_avg_embedding_map[token]) \t\t\n",
    "\t\telse:\n",
    "\t\t\tembedding += np.array(avg_token_embedding)\n",
    "\treturn embedding / float(len(tokens))\n",
    "\n",
    "\n",
    "def generate_word_embedding_map(words: list) -> dict:\n",
    "\tword_embedding_map = {}\n",
    "\tprocessed_words = 0\n",
    "\tfor word in words:\n",
    "\t\tembedding = get_average_embedding(word)\n",
    "\n",
    "\t\t# if word in word_embedding_map:\n",
    "\t\t\t# word_embedding_map[word] = np.mean(np.array([embedding, word_embedding_map[word]]), axis=0)\n",
    "\t\t# else:\n",
    "\t\tword_embedding_map[word] = embedding\n",
    "\t\n",
    "\t\tprocessed_words += 1\n",
    "\t\tprint(f\"{processed_words / len(words) * 100.0}% complete. {len(word_embedding_map)} word embeddings generated.\\t\\t\\t\", end ='\\r')\n",
    "\treturn word_embedding_map\n",
    "\n",
    "\n",
    "def load_words(from_file: str) -> list:\n",
    "\twords = []\n",
    "\n",
    "\twith open(from_file, 'r') as file:\n",
    "\t\twhile line := file.readline():\n",
    "\t\t\twords += [line.strip()]\n",
    "\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d6dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete. 400000 word embeddings generated.\t\t\tenerated.\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "words = load_words(\"glove.6B.300d-vocabulary.txt\")\n",
    "word_to_embedding = generate_word_embedding_map(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0a61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    if word in word_to_embedding:\n",
    "        emb = word_to_embedding[word]\n",
    "    else:\n",
    "        emb = get_average_embedding(word)\n",
    "        word_to_embedding[word] = emb\n",
    "    return emb\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    emb = get_word_embedding(word)\n",
    "\n",
    "    # calculate similarities to all words in out vocabulary\n",
    "    similarities = []\n",
    "    for word, embedding, in word_to_embedding.items():\n",
    "        similarity = embedding @ emb\n",
    "\n",
    "        similarities += [(float(similarity), str(word))]\n",
    "\n",
    "    similarities.sort(key = itemgetter(0))\n",
    "    similarities.reverse()\n",
    "    \n",
    "    return similarities[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee1ad",
   "metadata": {},
   "source": [
    "## 6 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52c9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(195.76938768723528, 'beings'),\n",
       " (195.04422725559118, 'worlds'),\n",
       " (194.98074462805857, 'place'),\n",
       " (193.96899141551464, 'tract'),\n",
       " (193.9049865194122, 'scene'),\n",
       " (193.82587903920148, 'offence'),\n",
       " (193.60080535435657, 'reader'),\n",
       " (193.50649510518875, 'decency'),\n",
       " (193.3968666057856, 'behaviour'),\n",
       " (193.19947584289272, 'affect')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72b0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(208.95104693764688, 'worlds'),\n",
       " (208.94183655681599, 'tract'),\n",
       " (208.88146745476683, 'beings'),\n",
       " (208.78977950702662, 'place'),\n",
       " (208.39590286223986, 'scene'),\n",
       " (207.48263558608045, 'reader'),\n",
       " (207.27289745687347, 'behaviour'),\n",
       " (207.23807727184376, 'decency'),\n",
       " (207.0258052612583, 'character'),\n",
       " (206.98893800820045, 'offence')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8a83d98-c0b3-405e-8de9-e21af3181d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(211.15260402327831, 'place'),\n",
       " (210.96373665527275, 'beings'),\n",
       " (210.730817087059, 'worlds'),\n",
       " (209.90084720089783, 'tract'),\n",
       " (209.84627423612548, 'reader'),\n",
       " (209.6598100137772, 'decency'),\n",
       " (209.63345684707483, 'scene'),\n",
       " (209.0384408161774, 'affect'),\n",
       " (209.0099198978825, 'offence'),\n",
       " (208.85120408214084, 'behaviour')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aca78ffe-22e4-46c2-9907-56beae22631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(208.50399005403946, 'place'),\n",
       " (208.209611689891, 'beings'),\n",
       " (208.12257918878376, 'worlds'),\n",
       " (207.3372136732168, 'tract'),\n",
       " (206.88450045115556, 'scene'),\n",
       " (206.09934818313025, 'decency'),\n",
       " (205.95078625559387, 'version'),\n",
       " (205.94334604545662, 'reader'),\n",
       " (205.61977261384394, 'affect'),\n",
       " (205.50517494157896, 'behaviour')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2232ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(214.17963296474522, 'place'),\n",
       " (214.17337795651963, 'beings'),\n",
       " (213.77337244797033, 'worlds'),\n",
       " (212.90681853370387, 'affect'),\n",
       " (212.59164426502696, 'tract'),\n",
       " (212.42115225710154, 'scene'),\n",
       " (212.3379639606195, 'decency'),\n",
       " (212.1175616283124, 'behaviour'),\n",
       " (212.03715577105646, 'offence'),\n",
       " (211.71178105354454, 'version')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "204a5c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(216.3175356073491, 'place'),\n",
       " (215.99651891442767, 'beings'),\n",
       " (215.65305980877426, 'worlds'),\n",
       " (214.99538814052298, 'tract'),\n",
       " (214.48374575602463, 'scene'),\n",
       " (214.43398872075582, 'decency'),\n",
       " (214.08270373607635, 'affect'),\n",
       " (213.91161270957855, 'reader'),\n",
       " (213.86771614943004, 'version'),\n",
       " (213.85424664491785, 'offence')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81126e70-c9ea-44d7-8f8e-55c3b872e1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
