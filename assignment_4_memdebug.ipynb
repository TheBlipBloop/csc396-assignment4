{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e382b",
   "metadata": {},
   "source": [
    "This notebook uses Roberta to generate a single dictionary which contains a mapping between a token (as a string) and a 756 dimensional averaged embedding over the provided text. The corpus to be used must be placed in the same directory as this notebook and named 'dataset.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report\n",
    "from operator import itemgetter\n",
    "import psutil\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset, sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 47820302 words.\n",
      "Average length: 67.34097665493726\n",
      "Max length: 3263\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0 \n",
    "\n",
    "lengths = []\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        sentences += [line]\n",
    "        linecount += 1\n",
    "        wordcount += len(line.split())\n",
    "        lengths += [len(line)]\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")\n",
    "print(\"Average length: \" + str(np.average(lengths)))\n",
    "print(\"Max length: \" + str(np.max(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a fast tokenizer,\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008bcd9",
   "metadata": {},
   "source": [
    "Quick sanity check to ensure tokenizer is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cfa809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pictures\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[1]\n",
    "tokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = 128, truncation = True, return_tensors='pt')\n",
    "\n",
    "# tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\t# padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "## Maybe reduce max length to 256 \n",
    "ids = tokens['input_ids'][0]\n",
    "mask = tokens['attention_mask'][0]\n",
    "print(tokenizer.decode(ids[7]))\n",
    "# sentences = sentences[:10_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4d75c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46400f",
   "metadata": {},
   "source": [
    "We'll be handling tokenization in a Dataset so we can take advantage of the DataLoader for auto batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a70e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RobertaDataset(Dataset):\n",
    "\tdef __init__(self, sentences: list, max_length: int):\n",
    "\t\tsentences_tokenized = []\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\ttokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = max_length, truncation = True, return_tensors='pt')\n",
    "\t\t\t\n",
    "\t\t\tids = torch.LongTensor(tokens['input_ids'][0])\n",
    "\t\t\tmask = torch.LongTensor(tokens['attention_mask'][0]) \n",
    "\n",
    "\t\t\tsentences_tokenized += [np.array([ids, mask])]\n",
    "\n",
    "\t\t\tprint(f\"{len(sentences_tokenized) / len(sentences) * 100.0}% complete.\\t\\t\\t\", end ='\\r')\n",
    "\n",
    "\t\tself.sentences_tokenized = np.array(sentences_tokenized)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences_tokenized)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn (self.sentences_tokenized[index][0], self.sentences_tokenized[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73684ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete.\t\t\tcomplete.\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "dataset = RobertaDataset(sentences, 128)\n",
    "BATCH_SIZE = 64\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\n",
    "\n",
    "sentence_count = len(sentences)\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5210ae",
   "metadata": {},
   "source": [
    "## Embedding Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Calculate a single embedding just to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdf7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "ids = batch[0].to(device)\n",
    "mask = batch[1].to(device)\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(ids, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7db6",
   "metadata": {},
   "source": [
    "Calculate the embeddings of our batches. I abort once 12_500 tokens have been collected as I start to run out of RAM after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aborting embedding generation early to avoid running out of RAM!AM utilization. \t\t\t\t\t\t\n",
      "78.949474304 GB used.\n",
      "3.253830704939218% complete. 43394 embeddings generated. 95.1% RAM utilization.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "import sys, traceback, gc\n",
    "type, val, tb = None, None, None\n",
    "type, val, tb = sys.exc_info()\n",
    "traceback.clear_frames(tb)\n",
    "\n",
    "token_to_avg_embedding_map = dict()\n",
    "avg_token_embedding = None\n",
    "\n",
    "def calculate_embeddings() -> dict:\n",
    "\tprocessed_sentances = 0\n",
    "\t\n",
    "\tmodel = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\ttoken_to_avg_embedding_map = {}\n",
    "\tavg_token_embedding = None\n",
    "\t\n",
    "\tembedding_range_cache = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\t\n",
    "\t\t\tids = batch[0].to(device)\n",
    "\t\t\tmask = batch[1].to(device)\n",
    "\t\t\t\n",
    "\t\t\toutput = model(ids, mask)\n",
    "\t\n",
    "\t\t\t####################################################################\n",
    "\t\t\n",
    "\t\t\t### shape, [batch, tokens in sentance, embeddings of each token]\n",
    "\t\t\tembeddings = output[0].detach().cpu() #.numpy()\n",
    "\t\t\tdel output\n",
    "\t\t\t\n",
    "\t\t\t# Update average embeddings, \n",
    "\t\t\tsentence_embedding_index = 0\n",
    "\t\t\twhile sentence_embedding_index < len(embeddings):\n",
    "\t\t\t# for sentence_embedding_index in range(len(embeddings)):\n",
    "\t\t\t\tsentence_embedding = embeddings[sentence_embedding_index]\n",
    "\t\n",
    "\t\t\t\ttoken_index = 0\n",
    "\t\t\t\twhile token_index < len(sentence_embedding):\n",
    "\t\t\t\t# for token_index in range(len(sentence_embedding)):\n",
    "\t\t\t\t\ttoken = ids[sentence_embedding_index][token_index]\n",
    "\t\t\t\t\ttoken_str = tokenizer.decode(token)\n",
    "\t\t\t\t\ttoken_embedding = sentence_embedding[token_index]\n",
    "\t\n",
    "\t\t\t\t\tif avg_token_embedding is None:\n",
    "\t\t\t\t\t\tavg_token_embedding = token_embedding\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tavg = np.array([avg_token_embedding, token_embedding])\n",
    "\t\t\t\t\t\tavg_token_embedding = np.mean(avg, axis=0)\n",
    "\t\t\t\t\t\tdel avg\n",
    "\t\n",
    "\t\t\t\t\tif token_str in token_to_avg_embedding_map:\n",
    "\t\t\t\t\t\tavg = np.array([token_to_avg_embedding_map[token_str], token_embedding])\n",
    "\t\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = np.mean(avg, axis=0)\n",
    "\t\t\t\t\t\tdel avg\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = token_embedding\n",
    "\n",
    "\t\t\t\t\ttoken_index += 1\n",
    "\t\t\t\t\tdel token\n",
    "\t\t\t\t\tdel token_str\n",
    "\t\t\t\t\tdel token_embedding\n",
    "\n",
    "\t\t\t\tsentence_embedding_index += 1\n",
    "\t\t\t\tdel sentence_embedding\n",
    "\t\n",
    "\t\t\tdel embeddings\n",
    "\t\t\tdel ids\n",
    "\t\t\tdel mask\n",
    "\t\t\tdel batch # Prevents memory leaks!!! this took a VERY long time to track down\n",
    "\t\t\n",
    "\t\t\tif (psutil.virtual_memory().percent) > 95.0:\n",
    "\t\t\t\tprint(\"Aborting embedding generation early to avoid running out of RAM!\")\n",
    "\t\t\t\tprint(f\"{psutil.virtual_memory().used / 1e9} GB used.\")\n",
    "\t\t\t\tprint(f\"{processed_sentances / (sentence_count) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization.\")\n",
    "\t\t\t\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\t\t\t\n",
    "\t\t\tprocessed_sentances += BATCH_SIZE\n",
    "\t\t\tprint(f\"{processed_sentances / (sentence_count) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization. \\t\\t\\t\", end ='\\r')\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\n",
    "token_to_avg_embedding_map, avg_token_embedding = calculate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cf766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4684",
   "metadata": {},
   "source": [
    "# Problem one complete!\n",
    "The token_to_avg_embedding_map is a dictonary mapping between sub-word tokens and their average embedding in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fb3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : 43394\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens : \" + str(len(token_to_avg_embedding_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f5629",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "In this section we are going to implement the most_similar() functions from chp 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72312c",
   "metadata": {},
   "source": [
    "First, generate a word to embedding mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6827696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(word):\n",
    "\ttokens = tokenizer(word)['input_ids']\n",
    "\tembedding = np.zeros(768)\n",
    "\tfor token in tokens:\n",
    "\t\ttoken = tokenizer.decode(token) # get the string this token id represents\n",
    "\t\tif token in token_to_avg_embedding_map:\n",
    "\t\t\tembedding += np.array(token_to_avg_embedding_map[token]) \t\t\n",
    "\t\telse:\n",
    "\t\t\tembedding += avg_token_embedding\n",
    "\treturn embedding / float(len(tokens))\n",
    "\n",
    "\n",
    "def generate_word_embedding_map(words: list) -> dict:\n",
    "\tword_embedding_map = {}\n",
    "\tprocessed_words = 0\n",
    "\tfor word in words:\n",
    "\t\tembedding = get_average_embedding(word)\n",
    "\t\tword_embedding_map[word] = embedding\n",
    "\t\n",
    "\t\tprocessed_words += 1\n",
    "\t\tprint(f\"{processed_words / len(words) * 100.0}% complete. {len(word_embedding_map)} word embeddings generated.\\t\\t\\t\", end ='\\r')\n",
    "\treturn word_embedding_map\n",
    "\n",
    "\n",
    "def load_words(from_file: str) -> list:\n",
    "\twords = []\n",
    "\n",
    "\twith open(from_file, 'r') as file:\n",
    "\t\twhile line := file.readline():\n",
    "\t\t\twords += [line.strip()]\n",
    "\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d6dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete. 400000 word embeddings generated.\t\t\tenerated.\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "words = load_words(\"glove.6B.300d-vocabulary.txt\")\n",
    "word_to_embedding = generate_word_embedding_map(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0a61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    if word in word_to_embedding:\n",
    "        emb = word_to_embedding[word]\n",
    "    else:\n",
    "        emb = get_average_embedding(word)\n",
    "        word_to_embedding[word] = emb\n",
    "    return emb\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    emb = get_word_embedding(word)\n",
    "\n",
    "    # calculate similarities to all words in out vocabulary\n",
    "    similarities = []\n",
    "    for word, embedding, in word_to_embedding.items():\n",
    "        similarity = embedding @ emb\n",
    "\n",
    "        similarities += [(float(similarity), str(word))]\n",
    "\n",
    "    similarities.sort(key = itemgetter(0))\n",
    "    similarities.reverse()\n",
    "    \n",
    "    return similarities[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee1ad",
   "metadata": {},
   "source": [
    "## 6 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c52c9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(129.4298404593606, 'hematologist'),\n",
       " (129.3095482185425, 'self-destruction'),\n",
       " (129.2839131216873, 'things'),\n",
       " (129.16494454079495, 'hemostasis'),\n",
       " (128.92477827282045, 'objectivity'),\n",
       " (128.42289551072992, 'heist'),\n",
       " (128.37232191518987, 'he'),\n",
       " (128.34957879863578, 'heakes'),\n",
       " (128.22011759486068, 'user-space'),\n",
       " (128.19456066310892, 'byte')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c72b0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(133.258082889447, 'he'),\n",
       " (133.11954709180307, 'things'),\n",
       " (132.6839322763326, 'heakes'),\n",
       " (132.3819955793722, 'byte'),\n",
       " (132.03737921162462, 'hematologist'),\n",
       " (131.99011399642782, 'hemostasis'),\n",
       " (131.94705023436487, 'heist'),\n",
       " (131.76831916527433, 'self-destruction'),\n",
       " (131.5810195161631, 'graphologist'),\n",
       " (131.3416912871951, 'heeds')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8a83d98-c0b3-405e-8de9-e21af3181d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(133.07528772348599, 'he'),\n",
       " (132.9637883763039, 'things'),\n",
       " (132.51406906296017, 'hemostasis'),\n",
       " (132.3865976724091, 'heakes'),\n",
       " (132.3566886395853, 'hematologist'),\n",
       " (132.07205604991944, 'byte'),\n",
       " (131.9958376690678, 'heist'),\n",
       " (131.7139442326221, 'self-destruction'),\n",
       " (131.31083020558904, 'heeds'),\n",
       " (131.23328961447447, 'hewell')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aca78ffe-22e4-46c2-9907-56beae22631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(130.52249817357432, 'he'),\n",
       " (130.35054012907875, 'things'),\n",
       " (129.72066712244705, 'heakes'),\n",
       " (129.64591908826932, 'hemostasis'),\n",
       " (129.45633036755393, 'hematologist'),\n",
       " (129.4075502528554, 'byte'),\n",
       " (129.20781604170108, 'heist'),\n",
       " (128.78076056850324, 'self-destruction'),\n",
       " (128.68881325184643, 'heeds'),\n",
       " (128.61578623807935, 'hewell')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2232ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(134.38869580935088, 'things'),\n",
       " (134.19651025339, 'he'),\n",
       " (134.09235898663604, 'hemostasis'),\n",
       " (133.63884397702145, 'hematologist'),\n",
       " (133.42244463624868, 'heist'),\n",
       " (133.41676545127433, 'resultative'),\n",
       " (133.27544605985858, 'heakes'),\n",
       " (133.0721063738572, 'self-destruction'),\n",
       " (133.03911802369842, 'byte'),\n",
       " (132.59518516160387, 'objectivity')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204a5c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(138.77673033068695, 'he'),\n",
       " (137.83944298111643, 'things'),\n",
       " (137.3630193709027, 'heakes'),\n",
       " (136.94316095221643, 'byte'),\n",
       " (136.92617364851606, 'hemostasis'),\n",
       " (136.89631452263274, 'hematologist'),\n",
       " (136.8067716985104, 'heist'),\n",
       " (136.05760844186545, 'heeds'),\n",
       " (135.93121321690953, 'self-destruction'),\n",
       " (135.9068883998036, 'hectors')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81126e70-c9ea-44d7-8f8e-55c3b872e1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
