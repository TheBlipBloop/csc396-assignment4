{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e382b",
   "metadata": {},
   "source": [
    "This notebook uses Roberta to generate a single dictionary which contains a mapping between a token (as a string) and a 756 dimensional averaged embedding over the provided text. The corpus to be used must be placed in the same directory as this notebook and named 'dataset.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report\n",
    "from operator import itemgetter\n",
    "import psutil\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551ea27",
   "metadata": {},
   "source": [
    "Initalize global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "MAX_TOKENIZATION_LENGTH = 128\n",
    "\n",
    "\n",
    "# Model Specifcs\n",
    "MODEL = \"roberta-base\"\n",
    "EMBEDDING_SIZE = 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset, sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 47820302 words.\n",
      "Average length: 67.34097665493726\n",
      "Max length: 3263\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0 \n",
    "\n",
    "lengths = []\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        sentences += [line]\n",
    "        linecount += 1\n",
    "        wordcount += len(line.split())\n",
    "        lengths += [len(line)]\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")\n",
    "print(\"Average length: \" + str(np.average(lengths)))\n",
    "print(\"Max length: \" + str(np.max(lengths)))\n",
    "\n",
    "sentences = sentences[:25_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a fast tokenizer,\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4d75c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46400f",
   "metadata": {},
   "source": [
    "We'll be handling tokenization in a Dataset so we can take advantage of the DataLoader for auto batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a70e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RobertaDataset(Dataset):\n",
    "\tdef __init__(self, sentences: list, max_length: int):\n",
    "\t\tsentences_tokenized = []\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\ttokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = max_length, truncation = True, return_tensors='pt')\n",
    "\t\t\t\n",
    "\t\t\tids = torch.LongTensor(tokens['input_ids'][0])\n",
    "\t\t\tmask = torch.LongTensor(tokens['attention_mask'][0]) \n",
    "\n",
    "\t\t\tsentences_tokenized += [np.array([ids, mask])]\n",
    "\n",
    "\t\t\tprint(f\"{len(sentences_tokenized) / len(sentences) * 100.0}% complete.\\t\\t\\t\", end ='\\r')\n",
    "\n",
    "\t\tself.sentences_tokenized = np.array(sentences_tokenized)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences_tokenized)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn (self.sentences_tokenized[index][0], self.sentences_tokenized[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73684ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete.\t\t\t\tomplete.\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "dataset = RobertaDataset(sentences, MAX_TOKENIZATION_LENGTH)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\n",
    "\n",
    "sentence_count = len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5210ae",
   "metadata": {},
   "source": [
    "## Embedding Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Calculate a single embedding just to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f22c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained(MODEL).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7db6",
   "metadata": {},
   "source": [
    "Calculate the embeddings of our batches. I abort once 12_500 tokens have been collected as I start to run out of RAM after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.352% complete. 0 embeddings generated. 15.8% RAM utilization. \t\t\tation. \t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "token_size = tokenizer.vocab_size\n",
    "\n",
    "token_to_embedding_sums = np.zeros((token_size, EMBEDDING_SIZE))\n",
    "token_to_embedding_counts = np.zeros((token_size, 1))\n",
    "token_to_embedding_averages = np.zeros((token_size, EMBEDDING_SIZE))\n",
    "\n",
    "processed_tokens = 0;\n",
    "\n",
    "def calculate_embeddings(model) -> dict:\n",
    "\tprocessed_sentances = 0\n",
    "\t\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\ttoken_to_avg_embedding_map = {}\n",
    "\tavg_token_embedding = None\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\t\n",
    "\t\t\tids = batch[0].to(device)\n",
    "\t\t\tmask = batch[1].to(device)\n",
    "\t\t\t\n",
    "\t\t\toutput = model(ids, mask)\n",
    "\t\n",
    "\t\t\t####################################################################\n",
    "\t\t\n",
    "\t\t\t### shape, [batch, tokens in sentance, embeddings of each token]\n",
    "\t\t\tembeddings = output[0].detach().cpu().numpy()\n",
    "\t\t\t# del output\n",
    "\t\t\t\n",
    "\t\t\t# Update average embeddings, \n",
    "\t\t\tfor sentence_embedding_index in range(len(embeddings)):\n",
    "\t\t\t\tsentence_embedding = embeddings[sentence_embedding_index]\n",
    "\t\n",
    "\t\t\t\tfor token_index in range(len(sentence_embedding)):\n",
    "\t\t\t\t\ttoken = ids[sentence_embedding_index][token_index]\n",
    "\t\t\t\t\ttoken_to_embedding_sums[token] += sentence_embedding[token_index]\n",
    "\t\t\t\t\ttoken_to_embedding_counts[token] += 1\n",
    "\n",
    "\t\t\t# if (psutil.virtual_memory().percent) > 95.0:\n",
    "\t\t\t\t# print(\"Aborting embedding generation early to avoid running out of RAM!\")\n",
    "\t\t\t\t# print(f\"{psutil.virtual_memory().used / 1e9} GB used.\")\n",
    "\t\t\t\t# print(f\"{processed_sentances / (sentence_count) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization.\")\n",
    "\t\t\t\t# return token_to_avg_embedding_map, avg_token_embedding\n",
    "\n",
    "\t\t\tprocessed_sentances += BATCH_SIZE\n",
    "\t\t\tprint(f\"{processed_sentances / (sentence_count) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization. \\t\\t\\t\", end ='\\r')\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\n",
    "token_to_avg_embedding_map, avg_token_embedding = calculate_embeddings(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b878e",
   "metadata": {},
   "source": [
    "Post process embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6cf766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50265, 768)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "average_embedding = np.sum(token_to_embedding_averages, axis=0)\n",
    "\n",
    "token_to_embedding_counts[token_to_embedding_counts == 0] = 1 # Set all onencounterd to tokens to the average token!\n",
    "token_to_embedding_averages = token_to_embedding_sums / token_to_embedding_counts\n",
    "\n",
    "print(token_to_embedding_averages.shape)\n",
    "print(average_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4684",
   "metadata": {},
   "source": [
    "# Problem one complete!\n",
    "The token_to_avg_embedding_map is a dictonary mapping between sub-word tokens and their average embedding in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f5629",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "In this section we are going to implement the most_similar() functions from chp 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72312c",
   "metadata": {},
   "source": [
    "First, generate a word to embedding mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6827696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(word):\n",
    "\ttokens = tokenizer(word)['input_ids']\n",
    "\tembedding = np.zeros(768)\n",
    "\tfor token in tokens:\n",
    "\t\tembedding += token_to_embedding_averages[token]\t\n",
    "\treturn embedding / len(tokens)\n",
    "\n",
    "\n",
    "def generate_word_embedding_map(words: list) -> dict:\n",
    "\tword_embedding_map = {}\n",
    "\tprocessed_words = 0\n",
    "\tfor word in words:\n",
    "\t\tembedding = get_average_embedding(word)\n",
    "\t\tword_embedding_map[word] = embedding\n",
    "\t\n",
    "\t\tprocessed_words += 1\n",
    "\t\tprint(f\"{processed_words / len(words) * 100.0}% complete. {len(word_embedding_map)} word embeddings generated.\\t\\t\\t\", end ='\\r')\n",
    "\treturn word_embedding_map\n",
    "\n",
    "\n",
    "def load_words(from_file: str) -> list:\n",
    "\twords = []\n",
    "\n",
    "\twith open(from_file, 'r') as file:\n",
    "\t\twhile line := file.readline():\n",
    "\t\t\twords += [line.strip()]\n",
    "\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06d6dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete. 400000 word embeddings generated.\t\t\tenerated.\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "words = load_words(\"glove.6B.300d-vocabulary.txt\")\n",
    "word_to_embedding = generate_word_embedding_map(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb0a61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    if word in word_to_embedding:\n",
    "        emb = word_to_embedding[word]\n",
    "    else:\n",
    "        emb = get_average_embedding(word)\n",
    "        word_to_embedding[word] = emb\n",
    "    return emb\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    emb = get_word_embedding(word)\n",
    "\n",
    "    # calculate similarities to all words in our vocabulary\n",
    "    similarities = []\n",
    "    for word, embedding, in word_to_embedding.items():\n",
    "        similarity = embedding @ emb\n",
    "\n",
    "        similarities += [(float(similarity), str(word))]\n",
    "\n",
    "    similarities.sort(key = itemgetter(0))\n",
    "    similarities.reverse()\n",
    "    \n",
    "    return similarities[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee1ad",
   "metadata": {},
   "source": [
    "## 6 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c52c9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(97.38421753517841, 'higher-dimensional'),\n",
       " (97.18366326018737, 'low-dimensional'),\n",
       " (97.12702066152566, 'high-dimensional'),\n",
       " (96.85361455321862, 'finite-dimensional'),\n",
       " (96.80543874486999, 'one-dimensional'),\n",
       " (96.67597460630914, 'other-dimensional'),\n",
       " (96.23280478211657, 'zero-dimensional'),\n",
       " (96.21962179810112, 'peoplehood'),\n",
       " (96.15755769409787, 'extra-dimensional'),\n",
       " (95.98457447005109, 'personhood')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c72b0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(86.33701600353515, 'vs'),\n",
       " (86.0279301507524, 'render'),\n",
       " (85.99913654675676, 'vs.'),\n",
       " (85.70931571741625, 'chip'),\n",
       " (85.6879070997179, 'linkage'),\n",
       " (85.61234521899873, 'hybrids'),\n",
       " (85.60077070207244, 'civilizations'),\n",
       " (85.47937375426555, 'worlds'),\n",
       " (85.20825110611888, 'operator'),\n",
       " (85.02999841686741, 'info')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8a83d98-c0b3-405e-8de9-e21af3181d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(129.47584800607376, 'higher-dimensional'),\n",
       " (128.99098028010266, 'high-dimensional'),\n",
       " (128.92776755697906, 'low-dimensional'),\n",
       " (128.75041768788282, 'other-dimensional'),\n",
       " (128.67045208907925, 'one-dimensional'),\n",
       " (128.59796138552187, 'finite-dimensional'),\n",
       " (128.31271997569348, 'peoplehood'),\n",
       " (127.94978559573272, 'zero-dimensional'),\n",
       " (127.91492941320858, 'personhood'),\n",
       " (127.79011016298041, 'extra-dimensional')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aca78ffe-22e4-46c2-9907-56beae22631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128.28140365471972, 'higher-dimensional'),\n",
       " (127.9407301410987, 'low-dimensional'),\n",
       " (127.90276048994124, 'high-dimensional'),\n",
       " (127.79083524427996, 'finite-dimensional'),\n",
       " (127.47068509943561, 'other-dimensional'),\n",
       " (127.47056365993181, 'one-dimensional'),\n",
       " (127.26570160834164, 'peoplehood'),\n",
       " (126.8517493097851, 'personhood'),\n",
       " (126.69591978955545, 'zero-dimensional'),\n",
       " (126.53893555251778, 'two-dimensional')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2232ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(134.45363308434554, 'higher-dimensional'),\n",
       " (133.8096768570758, 'low-dimensional'),\n",
       " (133.68422465659432, 'high-dimensional'),\n",
       " (133.5952588180821, 'one-dimensional'),\n",
       " (133.49855396129158, 'other-dimensional'),\n",
       " (133.29942964689556, 'finite-dimensional'),\n",
       " (132.69809617646814, 'extra-dimensional'),\n",
       " (132.5082375330686, 'zero-dimensional'),\n",
       " (132.22603416776482, 'two-dimensional'),\n",
       " (132.08797114550256, 'inter-dimensional')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a5c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(138.4307937488648, 'higher-dimensional'),\n",
       " (138.19154641536704, 'personhood'),\n",
       " (138.06559775379966, 'low-dimensional'),\n",
       " (137.9410631798128, 'high-dimensional'),\n",
       " (137.7783529078129, 'peoplehood'),\n",
       " (137.7600299442174, 'other-dimensional'),\n",
       " (137.69984696620025, 'one-dimensional'),\n",
       " (137.62260976405366, 'finite-dimensional'),\n",
       " (136.80963619398415, 'thing'),\n",
       " (136.73713751580357, 'zero-dimensional')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8adb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81126e70-c9ea-44d7-8f8e-55c3b872e1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
