{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e382b",
   "metadata": {},
   "source": [
    "This notebook uses Roberta to generate a single dictionary which contains a mapping between a token (as a string) and a 756 dimensional averaged embedding over the provided text. The corpus to be used must be placed in the same directory as this notebook and named 'dataset.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report\n",
    "from operator import itemgetter\n",
    "import psutil\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset, sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 47820302 words.\n",
      "Average length: 67.34097665493726\n",
      "Max length: 3263\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0 \n",
    "\n",
    "lengths = []\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        sentences += [line]\n",
    "        linecount += 1\n",
    "        wordcount += len(line.split())\n",
    "        lengths += [len(line)]\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")\n",
    "print(\"Average length: \" + str(np.average(lengths)))\n",
    "print(\"Max length: \" + str(np.max(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a fast tokenizer,\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008bcd9",
   "metadata": {},
   "source": [
    "Quick sanity check to ensure tokenizer is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cfa809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pictures\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[1]\n",
    "tokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = 128, truncation = True, return_tensors='pt')\n",
    "\n",
    "# tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\t# padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "## Maybe reduce max length to 256 \n",
    "ids = tokens['input_ids'][0]\n",
    "mask = tokens['attention_mask'][0]\n",
    "print(tokenizer.decode(ids[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4d75c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46400f",
   "metadata": {},
   "source": [
    "We'll be handling tokenization in a Dataset so we can take advantage of the DataLoader for auto batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a70e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RobertaDataset(Dataset):\n",
    "\tdef __init__(self, sentences: list, max_length: int):\n",
    "\t\tsentences_tokenized = []\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\ttokens = tokenizer.encode_plus(sentence, padding = \"max_length\", max_length = max_length, truncation = True, return_tensors='pt')\n",
    "\t\t\t\n",
    "\t\t\tids = torch.LongTensor(tokens['input_ids'][0])\n",
    "\t\t\tmask = torch.LongTensor(tokens['attention_mask'][0]) \n",
    "\n",
    "\t\t\tsentences_tokenized += [np.array([ids, mask])]\n",
    "\n",
    "\t\t\tprint(f\"{len(sentences_tokenized) / len(sentences) * 100.0}% complete.\\t\\t\\t\", end ='\\r')\n",
    "\n",
    "\t\tself.sentences_tokenized = np.array(sentences_tokenized)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences_tokenized)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn (self.sentences_tokenized[index][0], self.sentences_tokenized[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73684ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete.\t\t\tcomplete.\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "dataset = RobertaDataset(sentences, 128)\n",
    "BATCH_SIZE = 256\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5210ae",
   "metadata": {},
   "source": [
    "## Embedding Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Calculate a single embedding just to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdf7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "ids = batch[0].to(device)\n",
    "mask = batch[1].to(device)\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(ids, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7db6",
   "metadata": {},
   "source": [
    "Calculate the embeddings of our batches. I abort once 12_500 tokens have been collected as I start to run out of RAM after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aborting embedding generation early to avoid running out of RAM!RAM utilization. \t\t\t\t\t\n",
      "79.212089344 GB used.\n",
      "3.8725168248924495% complete. 44187 embeddings generated. 95.1% RAM utilization.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "import sys, traceback, gc\n",
    "type, val, tb = None, None, None\n",
    "type, val, tb = sys.exc_info()\n",
    "traceback.clear_frames(tb)\n",
    "\n",
    "token_to_avg_embedding_map = dict()\n",
    "avg_token_embedding = None\n",
    "\n",
    "def calculate_embeddings() -> dict:\n",
    "\tprocessed_sentances = 0\n",
    "\t\n",
    "\tmodel = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\ttoken_to_avg_embedding_map = {}\n",
    "\tavg_token_embedding = None\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\t\n",
    "\t\t\tids = batch[0].to(device)\n",
    "\t\t\tmask = batch[1].to(device)\n",
    "\t\t\t\n",
    "\t\t\toutput = model(ids, mask)\n",
    "\t\n",
    "\t\t\t####################################################################\n",
    "\t\t\n",
    "\t\t\t### shape, [batch, tokens in sentance, embeddings of each token]\n",
    "\t\t\tembeddings = output[0].detach().cpu().numpy()\n",
    "\t\n",
    "\t\t\t\n",
    "\t\t\t# Update average embeddings, \n",
    "\t\t\tfor sentence_embedding_index in range(len(embeddings)):\n",
    "\t\t\t\tsentence_embedding = embeddings[sentence_embedding_index]\n",
    "\t\n",
    "\t\t\t\tfor token_index in range(len(sentence_embedding)):\n",
    "\t\t\t\t\ttoken = ids[sentence_embedding_index][token_index]\n",
    "\t\t\t\t\ttoken_str = tokenizer.decode(token)\n",
    "\t\t\t\t\ttoken_embedding = sentence_embedding[token_index]\n",
    "\t\n",
    "\t\t\t\t\tif avg_token_embedding is None:\n",
    "\t\t\t\t\t\tavg_token_embedding = token_embedding\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tavg = np.array([avg_token_embedding, token_embedding])\n",
    "\t\t\t\t\t\tavg_token_embedding = np.mean(avg, axis=0)\n",
    "\t\n",
    "\t\t\t\t\tif token_str in token_to_avg_embedding_map:\n",
    "\t\t\t\t\t\tavg = np.array([token_to_avg_embedding_map[token_str], token_embedding])\n",
    "\t\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = np.mean(avg, axis=0)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttoken_to_avg_embedding_map[token_str] = token_embedding\n",
    "\t\n",
    "\t\t\tdel embeddings\n",
    "\t\t\tdel ids\n",
    "\t\t\tdel mask\n",
    "\t\t\tdel batch # Prevents memory leaks!!! this took a VERY long time to track down\n",
    "\t\t\n",
    "\t\t\tif (psutil.virtual_memory().percent) > 95.0:\n",
    "\t\t\t\tprint(\"Aborting embedding generation early to avoid running out of RAM!\")\n",
    "\t\t\t\tprint(f\"{psutil.virtual_memory().used / 1e9} GB used.\")\n",
    "\t\t\t\tprint(f\"{processed_sentances / len(sentences) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization.\")\n",
    "\t\t\t\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\t\t\t\n",
    "\t\t\tprocessed_sentances += BATCH_SIZE\n",
    "\t\t\tprint(f\"{processed_sentances / len(sentences) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated. {psutil.virtual_memory().percent}% RAM utilization. \\t\\t\\t\", end ='\\r')\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "\n",
    "token_to_avg_embedding_map, avg_token_embedding = calculate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cf766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4684",
   "metadata": {},
   "source": [
    "# Problem one complete!\n",
    "The token_to_avg_embedding_map is a dictonary mapping between sub-word tokens and their average embedding in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fb3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : 44187\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens : \" + str(len(token_to_avg_embedding_map)))\n",
    "avg_token_embedding = avg_token_embedding.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f5629",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "In this section we are going to implement the most_similar() functions from chp 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72312c",
   "metadata": {},
   "source": [
    "First, generate a word to embedding mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6827696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(word):\n",
    "\ttokens = tokenizer(word)['input_ids']\n",
    "\tembedding = np.zeros(768)\n",
    "\tfor token in tokens:\n",
    "\t\ttoken = tokenizer.decode(token) # get the string this token id represents\n",
    "\t\tif token in token_to_avg_embedding_map:\n",
    "\t\t\tembedding += np.array(token_to_avg_embedding_map[token]) \t\t\n",
    "\t\telse:\n",
    "\t\t\tembedding += np.array(avg_token_embedding)\n",
    "\treturn embedding / float(len(tokens))\n",
    "\n",
    "\n",
    "def generate_word_embedding_map(words: list) -> dict:\n",
    "\tword_embedding_map = {}\n",
    "\tprocessed_words = 0\n",
    "\tfor word in words:\n",
    "\t\tembedding = get_average_embedding(word)\n",
    "\t\tword_embedding_map[word] = embedding\n",
    "\t\n",
    "\t\tprocessed_words += 1\n",
    "\t\tprint(f\"{processed_words / len(words) * 100.0}% complete. {len(word_embedding_map)} word embeddings generated.\\t\\t\\t\", end ='\\r')\n",
    "\treturn word_embedding_map\n",
    "\n",
    "\n",
    "def load_words(from_file: str) -> list:\n",
    "\twords = []\n",
    "\n",
    "\twith open(from_file, 'r') as file:\n",
    "\t\twhile line := file.readline():\n",
    "\t\t\twords += [line.strip()]\n",
    "\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d6dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete. 400000 word embeddings generated.\t\t\tenerated.\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "words = load_words(\"glove.6B.300d-vocabulary.txt\")\n",
    "word_to_embedding = generate_word_embedding_map(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0a61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    if word in word_to_embedding:\n",
    "        emb = word_to_embedding[word]\n",
    "    else:\n",
    "        emb = get_average_embedding(word)\n",
    "        word_to_embedding[word] = emb\n",
    "    return emb\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    emb = get_word_embedding(word)\n",
    "\n",
    "    # calculate similarities to all words in out vocabulary\n",
    "    similarities = []\n",
    "    for word, embedding, in word_to_embedding.items():\n",
    "        similarity = embedding @ emb\n",
    "\n",
    "        similarities += [(float(similarity), str(word))]\n",
    "\n",
    "    similarities.sort(key = itemgetter(0))\n",
    "    similarities.reverse()\n",
    "    \n",
    "    return similarities[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee1ad",
   "metadata": {},
   "source": [
    "## 6 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c52c9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(135.17034697830746, 'part-owner'),\n",
       " (134.20062800382647, 'user-created'),\n",
       " (134.1277011555912, 'name-the-team'),\n",
       " (133.9714977573887, 'one-episode'),\n",
       " (133.93418796243938, 'one-party'),\n",
       " (133.89707581810606, 'anti-party'),\n",
       " (133.72221367501683, 'one-player'),\n",
       " (133.6491954480736, 'user-friendly'),\n",
       " (133.6461239500104, 'power-to-weight'),\n",
       " (133.61926210586958, 'one-term')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c72b0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(131.8084804904319, 'user-created'),\n",
       " (131.4068415488551, 'part-owner'),\n",
       " (131.30965294915973, 'one-episode'),\n",
       " (130.95716648049793, 'one-player'),\n",
       " (130.91848643973148, 'real-valued'),\n",
       " (130.8722229835089, 'name-the-team'),\n",
       " (130.82019322180255, 'one-party'),\n",
       " (130.8113679663642, 'game-like'),\n",
       " (130.66150368793814, 'user-friendly'),\n",
       " (130.62885660673535, 'one-track')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8a83d98-c0b3-405e-8de9-e21af3181d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(133.55712045509165, 'user-created'),\n",
       " (133.26061048682678, 'part-owner'),\n",
       " (132.94335490381417, 'one-episode'),\n",
       " (132.66173470091167, 'user-friendly'),\n",
       " (132.59729716430834, 'name-the-team'),\n",
       " (132.46072966834174, 'one-party'),\n",
       " (132.45511141290132, 'one-player'),\n",
       " (132.38382017421995, 'anti-party'),\n",
       " (132.3580995778959, 'real-valued'),\n",
       " (132.16194029413293, 'one-track')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aca78ffe-22e4-46c2-9907-56beae22631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(133.0994850243323, 'part-owner'),\n",
       " (132.57928091089403, 'user-created'),\n",
       " (131.9128229380362, 'user-friendly'),\n",
       " (131.7707501294017, 'name-the-team'),\n",
       " (131.73259254899355, 'one-episode'),\n",
       " (131.6786988028314, 'one-party'),\n",
       " (131.6127170847452, 'great-great-great'),\n",
       " (131.61109384726643, 'one-player'),\n",
       " (131.47443573424695, 'one-term'),\n",
       " (131.46982881884452, 'anti-party')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2232ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(137.42207584200807, 'part-owner'),\n",
       " (136.3310649736624, 'user-created'),\n",
       " (136.29527859413733, 'one-episode'),\n",
       " (135.8783439587999, 'one-party'),\n",
       " (135.8567239402024, 'name-the-team'),\n",
       " (135.70504052564243, 'one-player'),\n",
       " (135.70073002612594, 'one-term'),\n",
       " (135.43609344005472, 'anti-party'),\n",
       " (135.42801931177303, 'user-friendly'),\n",
       " (135.39107954216493, 'one-make')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204a5c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(140.87658893337073, 'part-owner'),\n",
       " (140.22962590297868, 'name-the-team'),\n",
       " (140.02311148411118, 'user-created'),\n",
       " (139.79206154958516, 'one-episode'),\n",
       " (139.52758815753043, 'one-term'),\n",
       " (139.44491825401548, 'one-player'),\n",
       " (139.43042353283406, 'one-party'),\n",
       " (139.26150132424408, 'user-friendly'),\n",
       " (139.25740987244, 'anti-party'),\n",
       " (139.13074353559648, 'one-dimensional')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81126e70-c9ea-44d7-8f8e-55c3b872e1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
