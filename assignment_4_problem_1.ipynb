{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216965d0-503f-4267-9420-ee2cb6c130e8",
   "metadata": {},
   "source": [
    "# Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e382b",
   "metadata": {},
   "source": [
    "This notebook uses Roberta to generate a single dictionary which contains a mapping between a token (as a string) and a 756 dimensional averaged embedding over the provided text. The corpus to be used must be placed in the same directory as this notebook and named 'dataset.txt'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2746e-2836-44b9-8422-33ad1d30d0b7",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874bd9-a81e-419a-9ee5-2731ef1cafd1",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d805de2-295a-485c-abeb-d253c9c7e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ML libaries\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# RobertaModel and Tockenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70527ac9-3e10-41d0-8ffa-03eefbe50ffd",
   "metadata": {},
   "source": [
    "Initialize environment with GPU (or CPU as fallback!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e09e291-92dd-4179-9ded-72c95279a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821fb0a-89e7-44e2-a925-e2b12ac9b3b2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb98247-e9f4-4a9d-99c0-345246b40543",
   "metadata": {},
   "source": [
    "Load in dataset, sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c245daa-814e-4b8a-9b12-ebd4cecc0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4468825 lines and 47820302 words.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "linecount = 0\n",
    "wordcount = 0\n",
    "\n",
    "with open(\"dataset.txt\", 'r') as dataset_file:\n",
    "    while line := dataset_file.readline():\n",
    "        sentences += [line]\n",
    "        linecount += 1\n",
    "        wordcount += len(line.split())\n",
    "\n",
    "print(\"Loaded \" + str(linecount) + \" lines and \" + str(wordcount) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ec39-cc5e-448f-95bd-c59361dc084d",
   "metadata": {},
   "source": [
    "Initialize tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f85109f-f7ab-4780-9ba1-7c1799959a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a fast tokenizer,\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space = True, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008bcd9",
   "metadata": {},
   "source": [
    "Quick sanity check to ensure tokenizer is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cfa809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pictures\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[1]\n",
    "tokens = tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\tpadding=\"max_length\", max_length=256, truncation=True)\n",
    "## Maybe reduce max length to 256 \n",
    "ids = tokens['input_ids'][0]\n",
    "mask = tokens['attention_mask'][0] \n",
    "print(tokenizer.decode(ids[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4d75c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46400f",
   "metadata": {},
   "source": [
    "We'll be handling tokenization in a Dataset so we can take advantage of the DataLoader for auto batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a70e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RobertaDataset(Dataset):\n",
    "\tdef __init__(self, sentances: list, tokenizer_instance: object, max_length: int):\n",
    "\t\tself.tokenizer = tokenizer_instance\n",
    "\t\tself.max_length = max_length\n",
    "\t\tself.sentences = sentences\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sentences)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tsentence = self.sentences[index]\n",
    "\t\ttokens = self.tokenizer(sentence, is_split_into_words = True, return_tensors='pt', \\\n",
    "\t\t\t\t\t padding=\"max_length\", max_length=self.max_length, truncation=True)\n",
    "\t\tids = tokens['input_ids'][0]\n",
    "\t\tmask = tokens['attention_mask'][0] \n",
    "\t\treturn (torch.LongTensor(ids), torch.LongTensor(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73684ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RobertaDataset(sentences, tokenizer, 256)\n",
    "BATCH_SIZE = 128\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5210ae",
   "metadata": {},
   "source": [
    "## Embedding Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137620ff-40ae-4ba6-8874-86e576bfdb6d",
   "metadata": {},
   "source": [
    "Calculate a single embedding just to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdf7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "ids = batch[0].to(device)\n",
    "mask = batch[1].to(device)\n",
    "model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(ids, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7db6",
   "metadata": {},
   "source": [
    "Calculate the embeddings of our batches. I abort once 12_500 tokens have been collected as I start to run out of RAM after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5eb7cbd-4083-4edb-902f-b1cd61bd8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting early to avoid running out of RAM!dings generated.\t\t\t\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "import sys, traceback, gc\n",
    "type, val, tb = None, None, None\n",
    "type, val, tb = sys.exc_info()\n",
    "traceback.clear_frames(tb)\n",
    "\n",
    "def calculate_embeddings() -> dict:\n",
    "    processed_sentances = 0\n",
    "    \n",
    "    model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    token_to_avg_embedding_map = {}\n",
    "    avg_token_embedding = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \tfor batch in dataloader:\n",
    "    \n",
    "    \t\tids = batch[0].to(device)\n",
    "    \t\tmask = batch[1].to(device)\n",
    "    \n",
    "    \t\toutput = model(ids, mask)\n",
    "    \n",
    "    \t\t####################################################################\n",
    "    \t\n",
    "    \t\t### shape, [batch, tokens in sentance, embeddings of each token]\n",
    "    \t\tembeddings = output[0].detach().cpu()\n",
    "    \n",
    "    \t\t\n",
    "    \t\t# Update average embeddings, \n",
    "    \t\tfor sentence_embedding_index in range(len(embeddings)):\n",
    "    \t\t\tsentence_embedding = embeddings[sentence_embedding_index]\n",
    "    \n",
    "    \t\t\tfor token_index in range(len(sentence_embedding)):\n",
    "    \t\t\t\ttoken = ids[sentence_embedding_index][token_index]\n",
    "    \t\t\t\ttoken_str = tokenizer.decode(token)\n",
    "    \t\t\t\ttoken_embedding = sentence_embedding[token_index]\n",
    "    \n",
    "    \t\t\t\tif avg_token_embedding is None:\n",
    "    \t\t\t\t\tavg_token_embedding = token_embedding\n",
    "    \t\t\t\telse:\n",
    "    \t\t\t\t\tavg_token_embedding += token_embedding\n",
    "    \t\t\t\t\tavg_token_embedding /= 2\n",
    "\n",
    "                    # replace division by 2 with NP.average because that IS numerically stable!\n",
    "    \n",
    "    \t\t\t\tif token_str in token_to_avg_embedding_map:\n",
    "    \t\t\t\t\ttoken_to_avg_embedding_map[token_str] += token_embedding\n",
    "    \t\t\t\t\ttoken_to_avg_embedding_map[token_str] /= 2 # not stable but close enough\n",
    "    \t\t\t\telse:\n",
    "    \t\t\t\t\ttoken_to_avg_embedding_map[token_str] = token_embedding\n",
    "    \t\t\n",
    "    \t\tdel embeddings\n",
    "    \t\tdel ids\n",
    "    \t\tdel mask\n",
    "    \n",
    "    \t\tgc.collect()\n",
    "    \n",
    "    \t\tif len(token_to_avg_embedding_map) > 10_000:\n",
    "    \t\t\tprint(\"Exiting early to avoid running out of RAM!\")\n",
    "    \t\t\treturn token_to_avg_embedding_map, avg_token_embedding\n",
    "    \t\t\tbreak\n",
    "    \n",
    "    \t\tprocessed_sentances += BATCH_SIZE\n",
    "    \t\tprint(f\"{processed_sentances / len(sentences) * 100.0}% complete. {len(token_to_avg_embedding_map)} embeddings generated.\\t\\t\\t\", end ='\\r')\n",
    "\n",
    "token_to_avg_embedding_map, avg_token_embedding = calculate_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae4684",
   "metadata": {},
   "source": [
    "# Problem one complete!\n",
    "The token_to_avg_embedding_map is a dictonary mapping between sub-word tokens and their average embedding in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10fb3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : 10293\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens : \" + str(len(token_to_avg_embedding_map)))\n",
    "avg_token_embedding = avg_token_embedding.tolist()\n",
    "\n",
    "# print(token_to_avg_embedding_map['the'][:5])\n",
    "# print(token_to_avg_embedding_map['let'][:5])\n",
    "# print(token_to_avg_embedding_map['rea'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f5629",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "In this section we are going to implement the most_similar() functions from chp 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72312c",
   "metadata": {},
   "source": [
    "First, generate a word to embedding mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6827696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_average_embedding(word):\n",
    "\ttokens = tokenizer(word)['input_ids']\n",
    "\tembedding = np.array(avg_token_embedding)\n",
    "\tfor token in tokens:\n",
    "\t\ttoken = tokenizer.decode(token) # get the string this token id represents\n",
    "\t\tif token in token_to_avg_embedding_map:\n",
    "\t\t\tembedding += np.array(token_to_avg_embedding_map[token]) \t\t\n",
    "\t\telse:\n",
    "\t\t\tembedding += np.array(avg_token_embedding)\n",
    "\treturn embedding / float(len(tokens))\n",
    "\n",
    "def generate_word_embedding_map(words: list) -> dict:\n",
    "\tword_embedding_map = {}\n",
    "\tprocessed_words = 0\n",
    "\tfor word in words:\n",
    "\t\tembedding = get_average_embedding(word)\n",
    "\n",
    "\t\tif word in word_embedding_map:\n",
    "\t\t\tword_embedding_map[word] += embedding\n",
    "\t\t\tword_embedding_map[word] /= 2\n",
    "\t\telse:\n",
    "\t\t\tword_embedding_map[word] = embedding\n",
    "\t\n",
    "\t\tprocessed_words += 1\n",
    "\t\tprint(f\"{processed_words / len(words) * 100.0}% complete. {len(word_embedding_map)} word embeddings generated.\\t\\t\\t\", end ='\\r')\n",
    "\treturn word_embedding_map\n",
    "\n",
    "def load_words(from_file: str) -> list:\n",
    "\twords = []\n",
    "\n",
    "\twith open(from_file, 'r') as file:\n",
    "\t\twhile line := file.readline():\n",
    "\t\t\twords += [line.strip()]\n",
    "\n",
    "\treturn words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06d6dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% complete. 400000 word embeddings generated.\t\t\tenerated.\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "words = load_words(\"glove.6B.300d-vocabulary.txt\")\n",
    "word_to_embedding = generate_word_embedding_map(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0a61b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(233.6513370716967, 'person'),\n",
       " (232.64613655882204, 'term'),\n",
       " (232.3005480258835, 'destroy'),\n",
       " (232.10033507778545, 'parameter'),\n",
       " (231.89816562725494, 'consisted'),\n",
       " (231.8813921823355, 'things'),\n",
       " (231.87471472138853, '1978'),\n",
       " (231.42188572610095, 'significance'),\n",
       " (231.34554120770994, 'vehicle'),\n",
       " (231.32081220497497, 'characteristic')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "import copy\n",
    "\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    \n",
    "    # retrieve embedding for given word\n",
    "    if word in word_to_embedding:\n",
    "        emb = word_to_embedding[word]\n",
    "    else:\n",
    "        emb = avg_token_embedding\n",
    "    \n",
    "    # calculate similarities to all words in out vocabulary\n",
    "    similarities = []\n",
    "    for word, embedding, in word_to_embedding.items():\n",
    "        similarity = embedding @ emb\n",
    "\n",
    "        similarities += [(float(similarity), str(word))]\n",
    "\n",
    "    similarities.sort(key = itemgetter(0))\n",
    "    similarities.reverse()\n",
    "    \n",
    "    return similarities[:10]\n",
    "\n",
    "most_similar(\"water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52c9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(236.8912853624291, '1978'),\n",
       " (234.40151091817472, 'mediated'),\n",
       " (233.52853504437167, 'consisted'),\n",
       " (233.29640248163074, 'person'),\n",
       " (232.66125176828888, 'term'),\n",
       " (232.47942219713397, 'vs'),\n",
       " (231.7509706865751, '1902'),\n",
       " (231.68343644725837, 'myth'),\n",
       " (231.41147720392993, 'preached'),\n",
       " (231.38467474796846, 'universes')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"ear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c72b0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.42103363e-02,  2.12259841e-01, -3.10762102e-02,  1.44131978e-02,\n",
       "        3.02585393e-01,  1.62047781e-01,  8.79064873e-02,  1.37890449e-01,\n",
       "        3.89600905e-02, -9.25337821e-02, -1.50042788e-01,  2.67766441e-01,\n",
       "        4.85365962e-02, -1.63591256e-02, -5.52116583e-03,  7.14599689e-02,\n",
       "        1.59045286e-01,  1.08868358e-01, -4.03653911e-02,  1.02756780e-01,\n",
       "        3.34084034e-05,  2.61911744e-02, -5.67139288e-02,  1.70083928e-02,\n",
       "        2.70769293e-02,  9.80665796e-02,  1.16179866e-01, -9.32704707e-02,\n",
       "        1.96839665e-01,  1.28245577e-02, -1.80880542e-01, -5.40257518e-02,\n",
       "        1.43670632e-02,  6.14013771e-02, -4.13537795e-02,  7.38272642e-02,\n",
       "       -9.07509495e-02,  2.57223298e-02,  2.28458941e-01,  2.64816551e-02,\n",
       "       -2.47397040e-01, -1.34264464e-01,  5.35521749e-03,  7.49793183e-02,\n",
       "       -3.15548504e-02, -2.43499503e-02,  1.01703221e-01,  4.48265076e-02,\n",
       "       -4.35936699e-02,  7.89945821e-03,  8.45659074e-02, -4.38853353e-03,\n",
       "       -1.40682379e-01,  2.44252334e-02, -1.34213207e-01,  2.58015648e-01,\n",
       "       -2.44910568e-02,  2.06582579e-01,  4.71463551e-02,  1.30552550e-01,\n",
       "       -7.40111905e-02,  6.59264964e-01,  1.56683847e-02, -3.69771694e-04,\n",
       "        2.05956899e-01,  4.43955905e-02,  4.44113404e-02, -4.51902052e-03,\n",
       "        2.59169998e-02, -1.01048511e-01,  6.20870714e-02,  8.12247979e-02,\n",
       "        4.76941327e-03, -1.30047159e-02,  2.65624677e-02,  4.20944296e-02,\n",
       "        4.86706719e-02, -4.13397004e+00,  2.92598086e-01,  3.69610265e-03,\n",
       "        1.22625005e-01,  1.13834449e-01, -6.07973119e-01,  1.80405870e-01,\n",
       "       -3.79107042e-02,  3.35409225e-01,  1.27172458e-02,  1.03843428e-01,\n",
       "        1.93965749e-02,  4.12991562e-02, -1.28049172e-01,  4.80913917e-02,\n",
       "        2.65516295e-01,  1.67696111e-01, -1.48005687e-01, -1.89839731e-01,\n",
       "        3.07633206e-02,  1.49098958e+00,  8.80869726e-04,  1.09129556e-01,\n",
       "        1.38333767e-01,  6.03798156e-02, -4.29978917e-02,  7.85481092e-02,\n",
       "        1.18871655e-01,  1.52715430e-01,  1.36118211e-01,  2.35602781e-02,\n",
       "        9.81592539e-02,  2.79981146e-02, -1.79203217e-01, -1.57885554e-01,\n",
       "       -2.22523098e-01,  2.27500275e-02,  4.23182768e-02, -8.00468971e-02,\n",
       "        6.53658311e-02,  6.75414577e-02,  2.62180033e-03,  8.29895077e-02,\n",
       "        1.37940692e-02,  9.78361815e-02, -1.65671056e-01,  1.87377674e-01,\n",
       "       -8.55278571e-02,  1.05826787e-01,  1.78408007e-01, -2.08300432e-01,\n",
       "       -8.37680611e-02,  1.63586014e-01,  6.78198555e-02,  2.80508647e-02,\n",
       "        8.82359094e-03, -1.75048340e-01, -4.32941491e-02,  9.15369630e-02,\n",
       "       -1.89853370e-02,  6.75386215e-02,  2.40083655e-01, -1.26112644e-02,\n",
       "       -1.02865861e-01,  1.23832429e-01,  1.77041553e-02,  4.22963432e-02,\n",
       "        1.10059080e-01,  2.00669320e-03,  2.57148173e-01,  1.47354752e-01,\n",
       "        7.49354924e-02,  1.61974906e-01,  2.04863320e-01, -1.18348087e-01,\n",
       "       -4.79400965e-02, -3.97688691e-02,  6.72799746e-02, -1.65468653e-02,\n",
       "        2.42141013e-01, -1.78427761e-01, -2.18627932e-01,  1.36589058e-01,\n",
       "       -6.18832409e-02, -2.30872103e-01,  1.28930416e-02, -2.65664458e-02,\n",
       "       -5.57371068e-02, -1.01634295e-02,  1.61256517e-03,  5.73090712e-03,\n",
       "        1.35718125e-01,  6.44768107e-02,  2.65748628e-03,  5.66315899e-02,\n",
       "       -7.20863243e-02,  1.24634968e-01,  1.18845033e-01,  1.78729072e-01,\n",
       "       -8.68807981e-02,  1.17000062e-01,  2.42541730e-02,  1.68312381e-01,\n",
       "       -1.29929364e-01,  5.28422073e-02, -1.92114388e-01,  1.13747350e-01,\n",
       "        1.50588440e-02, -1.09833321e-01,  2.28676290e-02, -2.23033040e-01,\n",
       "        6.06552449e-02, -1.72684093e-02, -2.89465399e-01,  1.26594459e-01,\n",
       "        1.33771318e-01,  3.35196572e-02, -6.56950064e-02, -8.09012627e-02,\n",
       "        3.87609626e-04, -8.99512835e-02,  1.88588279e-01,  8.40553430e-02,\n",
       "        4.32744492e-02, -2.76949950e-01,  7.08741769e-02,  1.34672480e-01,\n",
       "       -1.81713723e-01,  1.28228916e-03, -3.08953645e-01,  1.43331279e-01,\n",
       "       -2.41426006e-02,  5.42165215e-03, -7.16511322e-02,  6.23542443e-02,\n",
       "       -1.37517269e-01, -1.29253268e-02, -7.90704850e-02, -1.49044114e-01,\n",
       "        5.74915713e-02, -1.55109081e+00, -5.77890848e-02, -3.54502911e-01,\n",
       "        3.42397578e-02,  8.32721932e-02, -1.18447899e-01,  1.37964370e-01,\n",
       "       -1.22689764e-01, -1.05724640e-01,  1.65584708e-02, -1.18393778e-01,\n",
       "        7.09335878e-02,  7.68735682e-02, -3.04609855e-02,  3.40935600e-02,\n",
       "       -8.86439731e-02, -5.16626431e-02, -1.03551249e-02, -1.63172786e-01,\n",
       "        2.13323648e-01,  7.56156072e-03,  1.09048527e-01,  8.74480555e-02,\n",
       "        6.75874611e-01, -8.03461075e-02,  1.61955662e-01,  9.15139057e-02,\n",
       "        1.08867990e-01,  1.29524683e-01, -5.62190364e-02, -1.75628881e-01,\n",
       "        1.10168944e-01,  2.35878956e-02, -2.41369338e-02, -4.12263585e-02,\n",
       "        1.08742927e-01, -3.87597164e-02,  3.37133543e-01, -6.23358969e-02,\n",
       "       -2.16883953e-02,  3.74291688e-02, -1.10945607e-01,  4.00784676e-01,\n",
       "        3.52494824e-02, -9.33738177e-02, -2.23295192e-02,  2.21225263e-01,\n",
       "       -1.35146846e-01, -1.33923891e-01, -3.03017410e-02,  2.64082791e-03,\n",
       "       -3.67675647e-02, -1.71234674e-01,  7.66118144e-02,  1.58478857e-01,\n",
       "       -7.71517778e-02,  2.38202276e-02,  5.75169747e-02,  5.34013808e-02,\n",
       "       -1.25351019e-01, -1.91940445e-03, -9.36035514e-02, -1.61208518e-01,\n",
       "        3.95745883e-01,  1.41191142e-01, -6.22190088e-02,  2.50968212e-01,\n",
       "       -1.89889029e-01, -1.02334853e-01, -1.71328697e-01,  8.02122350e-02,\n",
       "       -7.45452838e-02,  9.49999665e-02, -4.81023490e-02,  9.98691941e-02,\n",
       "       -7.67604883e-02,  1.39734705e-01,  5.82524342e-02,  2.23342653e-01,\n",
       "        7.72776107e-02, -1.21158596e-01,  2.45590360e-01,  4.89472970e-02,\n",
       "        1.57712685e-01, -1.36796772e-01, -5.78356534e-03, -8.35425562e-03,\n",
       "       -6.32234917e-02,  1.84942804e-01,  8.90016087e-02,  5.82432238e-02,\n",
       "        1.79081347e-01, -4.39215750e-02,  2.87278897e-02,  2.82472502e-01,\n",
       "       -8.41725866e-02,  1.21305419e-01,  1.80034268e-01,  5.12156455e-02,\n",
       "       -1.00450735e-01,  9.29884917e-02,  1.76547571e-02, -4.09484593e-02,\n",
       "        1.39485076e-02,  5.20085871e-02,  3.13425201e-02,  8.40299899e-02,\n",
       "        4.58991925e-02,  8.63732758e-02,  7.35070656e-02, -1.31132808e-01,\n",
       "       -1.74552632e-01,  1.65587398e-01,  7.89252922e-01, -1.23076526e+00,\n",
       "       -3.34187035e-02,  4.51618520e-02,  6.83740377e-02,  1.21560092e-01,\n",
       "        2.26675887e-02,  4.41722420e-02,  1.47073826e-01, -4.85834815e-02,\n",
       "       -2.78757475e-02,  1.47877486e-01, -8.38718452e-02, -3.97950883e-02,\n",
       "        3.82743202e-02,  5.22349402e-02, -1.35691142e-01,  4.16103924e-02,\n",
       "       -1.05008776e-01, -1.02400267e-01, -1.25141206e-01, -1.03411858e-01,\n",
       "       -2.11864082e-01, -5.73175525e-03, -2.44195378e-01,  1.19135792e-01,\n",
       "       -2.50121187e-02, -7.64676873e-02, -9.14901371e-02, -1.27633293e-01,\n",
       "        7.47983331e-02,  8.22289440e-02,  1.38614252e-01, -1.17996098e-01,\n",
       "        2.32771573e-01, -4.89882961e-01,  4.80433305e-03, -1.21406292e-01,\n",
       "       -1.47782861e-01,  5.40613408e-02,  1.84372626e-01,  1.39737693e-01,\n",
       "        1.82959223e-01,  7.96976329e-02, -4.20367440e-02, -3.48295718e-02,\n",
       "       -1.62225394e-01,  1.24540158e-02, -6.75946611e-02,  2.89980435e-02,\n",
       "       -5.15731337e-02,  2.67086521e-01,  3.51398413e-02, -9.62038313e-02,\n",
       "        1.58606010e-01, -3.13517302e-02, -1.10304145e-02, -3.04205871e-03,\n",
       "        5.54391081e-01,  1.36339702e-01, -3.82175061e-02,  2.27113782e-01,\n",
       "       -1.35018548e-02, -7.43020152e-02,  1.82085255e-02,  4.91496492e-02,\n",
       "       -1.60658732e-04,  6.23580560e-01,  5.33230236e-02, -7.98488905e-03,\n",
       "        2.89627770e-02, -7.98413480e-02, -4.87052028e-02,  2.23179794e-01,\n",
       "        5.79363039e-02,  9.97237265e-02,  1.62930444e-01, -6.95541588e-02,\n",
       "       -7.43449779e-02, -7.64394722e-02, -2.19492347e-02,  2.50185244e-02,\n",
       "        1.59192381e-01,  7.25932885e-02,  8.71828372e-02, -2.60586981e-02,\n",
       "        2.41847144e-02,  9.57856656e-02, -2.05690868e-01, -2.74353729e-01,\n",
       "       -9.24863899e-03, -1.20159369e-01, -1.12484129e-01,  2.36577518e-01,\n",
       "       -4.45897222e-01,  8.69971880e-02, -1.72366186e-01, -1.45353928e-01,\n",
       "       -7.57906971e-02, -1.24286977e-02,  1.12356801e-01,  7.14425196e-02,\n",
       "        3.84982998e-02,  3.27166344e-02,  5.94826310e-02, -1.52359804e-02,\n",
       "        8.39587537e-02, -1.60001697e-01, -1.52010494e-01,  6.79362640e-02,\n",
       "       -1.82414860e-01, -3.96335001e-02,  7.66112780e-04,  1.09601044e-01,\n",
       "        4.25216332e-02, -1.11354141e-01, -5.01240467e-02,  1.52159631e-02,\n",
       "       -2.56677330e-01,  1.28032605e-01, -1.54234554e-01,  1.54557279e-01,\n",
       "        1.45546303e-01, -4.13708486e+00,  8.96602795e-02,  1.46191008e-01,\n",
       "        6.09155822e-02,  7.35004867e-02, -3.18088544e-02,  1.36492833e-01,\n",
       "        1.87394209e-01,  4.70188018e-02,  2.26032337e-01, -7.76958155e-03,\n",
       "        8.08616454e-02, -3.06315565e-04,  7.28423571e-02, -1.55512471e-01,\n",
       "        3.79952394e-02,  1.86029159e-01,  3.30995594e-02,  8.02965121e-02,\n",
       "       -1.25092984e-01, -1.58546003e-01, -7.92112906e-02,  7.06243226e-02,\n",
       "       -8.46221570e-02, -6.73597058e-02, -6.28946349e-02, -2.13103648e-01,\n",
       "       -1.38353885e-02,  1.88253337e-01,  1.52947359e-01, -4.12517204e-02,\n",
       "       -4.45774732e-02,  3.92452062e-02, -2.06454876e-01,  7.18094471e-03,\n",
       "       -8.16481225e-02, -1.21302595e-01,  2.90905735e-01, -1.67270759e-02,\n",
       "        7.84566626e-03,  1.27488488e-01,  2.79229273e-01,  9.97488822e-02,\n",
       "        4.84095554e-01,  1.18951534e-01,  2.51418274e-01,  4.67242418e-02,\n",
       "        7.51359848e-02, -5.25294530e-02,  2.46095608e-02,  1.32970975e-01,\n",
       "       -1.30567923e-01,  4.10596168e-02,  3.16240191e-02,  4.94803364e-03,\n",
       "       -7.59632100e-03, -5.45612471e-02, -5.30697045e-02,  8.10801551e-02,\n",
       "       -3.05018574e-03,  9.43197079e-02,  1.83626538e-01,  1.47110981e-01,\n",
       "        1.67366852e-01,  3.69023085e-02, -2.39324996e-01, -2.41177636e-01,\n",
       "       -1.11430852e-03, -9.42444677e-02,  1.27807628e-01, -1.76930319e-02,\n",
       "       -7.11187820e-02, -2.85501157e-02, -7.69397008e-02, -6.40958476e-02,\n",
       "        5.82115750e-02, -2.36967041e-01, -1.24675181e-01, -1.46602278e-01,\n",
       "        2.98078088e-02, -8.36236610e-02,  4.94910423e-02,  6.09011998e-02,\n",
       "        1.23991550e-01, -2.18911479e-01,  1.94625601e-01, -8.43413348e-02,\n",
       "        4.79539422e-02,  2.78415603e-01, -8.17503929e-02, -1.24982676e-01,\n",
       "        3.93648067e-02, -9.38817058e-02,  1.20973162e-01,  1.10908226e-01,\n",
       "        9.64347025e-02,  1.70412944e-02, -1.97316562e-02, -5.08026451e-01,\n",
       "       -2.11162244e-02,  1.15395725e-01, -1.00142793e-01,  4.23099324e-02,\n",
       "       -3.09693664e-02,  7.36319845e-02, -1.94167420e-02,  2.48644948e-01,\n",
       "       -1.79000090e-01,  3.20968156e-02,  4.89118116e-02,  1.30283813e-01,\n",
       "       -1.03059366e-01,  1.47024957e-01,  2.22774354e-01,  7.52083883e-02,\n",
       "       -8.73033280e-02,  5.50707756e-03,  1.02069787e+00, -4.17123176e-02,\n",
       "       -1.25890800e-01,  4.87952273e-02,  1.88388504e-01, -4.91234350e-02,\n",
       "        1.06799704e-01, -7.46097843e-03, -1.33270646e-01,  1.42232200e-01,\n",
       "        2.25025922e-01, -5.85836985e-02,  4.15544002e-03, -3.25515258e-02,\n",
       "        2.51016350e-02,  3.13288209e-01,  2.42087577e-01,  2.27362696e-01,\n",
       "        1.30182616e+01, -1.58858549e-01,  8.80726402e-02,  6.92643340e-02,\n",
       "       -5.86458171e-03, -1.16205648e-01,  3.39495167e-02, -5.59412867e-03,\n",
       "       -5.43560994e-02,  7.95361772e-02,  1.31879016e-01, -8.24679149e-02,\n",
       "       -1.82085412e-01,  7.78333209e-02, -1.11834030e-02,  3.11585320e-02,\n",
       "        1.37807600e-01,  3.09732002e-01, -4.44557195e-02,  2.83881128e-02,\n",
       "       -1.54378662e-01,  3.40052365e-02,  8.85278968e-02, -4.92234943e-01,\n",
       "       -2.59775271e-02,  6.15015725e-02,  1.61191069e-01, -5.88091016e-02,\n",
       "        1.28819512e-01,  1.99211999e-01, -1.03406978e-02,  2.27960731e-01,\n",
       "        7.25117599e-02, -1.54469460e-02, -6.76280856e-02, -6.15336547e-02,\n",
       "       -6.00376718e-01,  2.95127432e-03,  1.77273611e-02, -1.37007236e-03,\n",
       "       -3.51944615e-02, -7.07529436e-02,  3.83795469e-02, -2.81301597e-01,\n",
       "        4.16850125e-02, -3.78839100e-02, -1.61212548e-01,  1.16347938e-01,\n",
       "       -1.91563753e-02, -1.11392937e-01,  5.51218241e-02,  3.99177894e-02,\n",
       "        1.98921510e-01, -9.37694230e-02,  4.94609348e-02, -3.65444327e-01,\n",
       "        1.37449113e-01,  2.08281614e-01,  1.90333761e-01, -1.58252464e-01,\n",
       "       -2.65020141e-01, -2.33331521e-02,  1.04445872e-02, -2.54746444e-01,\n",
       "       -2.23695938e-02,  1.50304014e-01,  9.69520211e-02, -1.46096754e-01,\n",
       "       -1.16361193e-01,  1.58448568e-01, -7.64184793e-02,  8.11087278e-02,\n",
       "        6.36487405e-02, -1.17324723e-02,  8.75060347e-02,  1.27525719e-02,\n",
       "       -2.06224034e-01, -1.44746898e-01, -7.19847629e-02, -5.04018786e-01,\n",
       "       -9.14454895e-02, -1.22693256e-02,  1.63926181e-01,  5.22520343e-02,\n",
       "       -1.13358450e-01,  2.17221652e-02, -3.50420891e-03,  2.47279753e-01,\n",
       "        1.66729213e-01,  4.81549626e-02, -2.39142614e-01, -7.37499123e-02,\n",
       "       -6.97224165e-02,  4.84309193e-02,  1.97056466e-01,  1.28497126e-01,\n",
       "        1.00302572e-01,  3.23279351e-02,  1.30591032e-01,  6.21065870e-02,\n",
       "       -1.72845145e-01,  4.33648589e-02, -8.89507867e-03, -3.17965299e-02,\n",
       "       -4.75495569e-02,  3.86323745e-01,  1.12429009e-01,  2.02259408e-01,\n",
       "       -4.76880955e-02,  1.31394413e-01,  2.23421110e-02,  8.82446890e-02,\n",
       "        6.40623315e-02, -2.14937935e-01, -2.32256932e-01,  8.35286248e-02,\n",
       "        1.76150522e-01,  2.07612844e-01,  1.37333257e-01,  7.07921262e-03,\n",
       "       -1.21540940e-01,  1.64024011e-01, -6.06938700e-02,  1.55459090e-01,\n",
       "        2.63460530e-02, -1.01589635e-01,  2.61425326e-01,  3.85135479e-02,\n",
       "       -2.09975732e-01,  3.91908834e-02, -2.63582026e-01,  7.98979501e-02,\n",
       "        2.07593893e-01, -2.02707897e-02,  5.28221329e-02,  1.46942047e-01,\n",
       "       -7.63391098e-03, -4.45806235e-03, -3.32917025e-02, -3.76661817e-02,\n",
       "        9.26690424e-02,  1.48282253e-01, -1.53858418e-01, -1.51319851e-02,\n",
       "       -7.44649892e-02, -1.32860086e-01,  8.32038944e-02,  4.07757843e-03,\n",
       "       -2.05020706e-02,  1.70579281e-01,  2.02439090e-01,  1.35389253e-02,\n",
       "        1.32419420e-01, -2.76009917e-01, -9.39221655e-02, -1.31232642e-01,\n",
       "       -5.51535723e-02, -2.42520372e-03,  6.10310072e-02, -1.24955510e-01,\n",
       "        1.55068065e-01, -7.32334773e-01, -2.02164271e-01,  1.68816532e-01,\n",
       "        5.75697757e-02,  1.97239033e-01,  1.75379318e-01, -1.37215364e-01,\n",
       "        2.54925976e-01,  3.35301065e-02, -1.83174415e-02,  5.65418651e-02,\n",
       "        3.05772324e-03,  4.85958382e-02, -2.77909519e-02,  2.54633807e-01,\n",
       "        2.23149009e-01, -1.65264005e-01, -1.06382787e-01, -1.58903427e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_embedding(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a83d98-c0b3-405e-8de9-e21af3181d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
